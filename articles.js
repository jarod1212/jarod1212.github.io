const articles = [
    {   title: "La nuance ne fragilise pas le propos",
            img: "articles_images/article1.png",
            text: `
            <p>En 1945, dans <em>The Crack-Up</em>, F. Scott Fitzgerald écrit : 
            « La marque d’une intelligence de premier ordre est la capacité de tenir simultanément deux idées opposées dans son esprit tout en conservant la capacité de fonctionner. »</p>

            <p>Être doctorant confère un statut un peu particulier : sans être reconnu comme un expert, nous sommes pourtant en première ligne. 
            Le métier de chercheur a cela d’original que l’on passe une majorité de notre temps à explorer des hypothèses fausses. 
            Pourtant il est de plus en plus dur de dire ouvertement “je ne sais pas”. On craint de ne plus être pris au sérieux. 
            Cela impacte l'ensemble du monde de la recherche. Trop de publications positives, pas assez de publications négatives. 
            Rester discipliné dans ses recherches, ne pas céder à la tentation d’enjoliver ses résultats et concéder parfois que l’on ne sait simplement pas est fondamental, 
            comme l’explique le journaliste Alexandre Morales dans son émission du 25 décembre sur France Culture [1]. 
            C’est le premier rempart à la désinformation. Car si la recherche triche, qui ne trichera pas ?</p>

            <p>À l'extérieur du monde de la recherche, la société semble avoir changé de configuration. 
            On encourage maintenant la prise de position tranchée, le cru, l'extrême. 
            Le public est-il encore à la recherche de la vérité ? 
            On sait déjà qu’il est beaucoup plus facile de répandre une fausse information qu’une vraie, 
            comme l’a démontré cet excellent article du MIT Media Lab [2].</p>

            <p>D’un point de vue cérébral, comment le cerveau appréhende-t-il le fait que la quantité d’information qui nous parvient a brutalement changé d’ordre de grandeur ? 
            Je me questionne. Combien de temps me suis-je accordé pour avoir un avis sur le nouveau gouvernement syrien ? 
            Et sur le nouveau Premier ministre français ? Pourquoi devrions-nous avoir un avis sur tout ? 
            Il semble parfois qu'être informé sans pour autant tout connaître soit devenu impensable. 
            Et changer de point de vue ? N’en parlons même pas !</p>

            <p>Je reprends ici largement les propos d'Étienne Klein, Ismaël Khelifa et d'autres qui prônent une nuance immodérée.</p>

            <p>Cela semble évident, c’est vrai. Selon la loi de Brandolini [3], 
            "la quantité d'énergie nécessaire pour réfuter des sottises [...] est supérieure d'un ordre de grandeur à celle nécessaire pour les produire."</p>

            <p>C'est fatigant, c'est vrai, mais ne laissons pas l'entièreté de la sphère publique aux positionnements schématiques.</p>

            <p class="subtle small">
            [1] Alexandre Morales – France Culture (2024).<br>
            [2] Vosoughi, S., Roy, D., & Aral, S. (2018). The spread of true and false news online. <em>Science</em>, 359(6380), 1146–1151.<br>
            [3] Bullshit Asymmetry Principle – Alberto Brandolini (2013).
            
            
            <p>        </p>
            
            
            </p>   `
      },
      {
        title: "L'apprentissage des langues ne doit pas être menacé",
        img: "articles_images/article2.png",
        text: `
          <p>À Noël, je discutais avec ma tante, professeure d’italien. Elle m’a fait part de ses inquiétudes concernant les avancées de l’IA et leurs répercussions sur l’apprentissage des langues.</p>
          <p>Apprendre une langue, est-ce déjà une pratique dépassée ? Si, dans le futur, on peut décoder en direct ce que l’autre nous dit, à quoi bon passer des années à apprendre une langue ?</p>
          <p>Pourtant, supprimer cet apprentissage sous prétexte qu’on peut avoir accès à une traduction immédiate est, pour beaucoup de neuroscientifiques, passer à côté de l’essentiel. L’objectif principal peut être de parler couramment, mais le processus d’apprentissage transforme surtout durablement le cerveau, même si la personne est incapable, aujourd’hui, d’aligner trois mots dans cette langue.</p>
          <p>Raphael Gaillard, psychiatre et académicien, en parle dans son livre <em>L’Homme Augmenté</em> [1]. Il écrit : « La culture ne vaut pas tant par ce que nous pouvons en restituer en termes de connaissances, mais par ce qu’il en reste une fois que nous avons tout oublié. » Cette idée est assez ancienne, on la trouve déjà à la fin du XIXᵉ siècle avec Ellen Key, pédagogue suédoise [2]. Ce que nous avons su un jour laisse une empreinte sur nous. Chaque moment passé à tenter d’apprendre quelque chose modifie notre réseau neuronal interne, et notre cerveau, bien qu’il puisse oublier, ne fonctionnera plus jamais comme avant cet apprentissage.</p>
          <p>L’important n’est donc pas de retenir, mais bien d’apprendre. D’autant plus qu’il existe maintenant un quasi-consensus sur le fait que les personnes qui ne cessent jamais d’apprendre ralentissent leur déclin cognitif et sont moins touchées par des maladies neurodégénératives comme Alzheimer [3]. Il ne faut jamais cesser d'apprendre.</p>
          <p>Dans un futur proche, les IA seront donc probablement utilisées massivement comme traducteurs instantanés. Cela ne doit cependant pas rimer avec l’abandon de l’apprentissage des langues. Car apprendre une langue, c’est se donner la chance de réfléchir dans un autre système. C’est également se donner la possibilité de penser comme l’autre, celui qui parle cette langue. C’est, avant tout, un acte social.</p>
          <p class="subtle small">
            [1] Raphael Gaillard. (2024). <em>L’Homme Augmenté</em>. Grasset.<br>
            [2] Ellen Key. (1891). <em>On tue l’esprit dans les écoles</em>. Revue Verdandi.<br>
            [3] Loef, M., & Walach, H. (2012). The combined effects of healthy lifestyle behaviors on all-cause mortality: A systematic review and meta-analysis. <em>Preventive Medicine</em>.
          </p>
        `
      },
      {
        title: "Les vertus de la lecture",
        img: "articles_images/article3.png",
        text: `
          <p>Cette semaine, je souhaitais partager une des études qui m’a le plus impressionné ces dernières années et qui montre encore une fois l'intérêt d'étudier la frontière entre neurosciences et intelligence artificielle. J’ai découvert ce résultat en lisant <em>L’Homme Augmenté</em> du neuropsychiatre Raphaël Gaillard [1] (remarquable livre !).</p>
          <p>Dans un podcast [2] du New England Journal of Medicine avec Peter Lee, VP research chez Microsoft, celui-ci mentionne un papier de Sébastien Bubeck [3]. Cet article rapporte une expérience où des chercheurs ont entraîné un réseau de neurones pour résoudre des systèmes d'équations linéaires à trois inconnues. Sans surprise, après l’entraînement, le modèle réussit à résoudre ce type de système, mais échoue face à des systèmes à quatre inconnues. Cependant, lorsque l’entraînement est répété en ajoutant aux données un corpus volumineux de textes non mathématiques, le modèle devient alors capable de résoudre n’importe quel système d'équations linéaires.</p>
          <p>Raphaël Gaillard commente ainsi : « Ces IA génératives, comme ChatGPT, nous montrent ce qui rend intelligent : la lecture. » Et à propos de l’article de Bubeck, il ajoute : « Le simple fait de lire permet au modèle d’atteindre un niveau de complexité supérieur en mathématiques. »</p>
          <p>Chez l’enfant, plusieurs études, notamment les travaux de Stanislas Dehaene [4], ont également démontré que les performances en lecture sont corrélées aux performances en mathématiques [5]. Il semblerait que de la même manière que le réseau de neurones artificiels, la lecture peut changer la conformation du cerveau pour mieux appréhender les mathématiques.</p>
          <p>Bref, tout cela donne envie de lire ! Voici trois règles qu’un grand lecteur m’a confiées, que j’applique depuis le début de l’année, et qui fonctionnent vraiment bien pour lire régulièrement (avant cela, je ne finissais presque aucun livre…) :</p>
          <ol>
            <li>Toujours avoir en tête le prochain livre qu’on a envie de lire pour ne pas perdre la dynamique.</li>
            <li>Si, après 50 pages, la lecture reste un effort, passer à un autre livre.</li>
            <li>À moins qu’il ne s’agisse d’une recommandation ou d’un chef-d’œuvre. Dans ce cas-là, on essaie encore ou on le met de côté pour plus tard.</li>
          </ol>
          <p class="subtle small">
            [1] Raphaël Gaillard. (2024). <em>L’Homme Augmenté</em>. Grasset. Page 314.<br>
            [2] NEJM Group (2024) - Microsoft’s Peter Lee on the Future of Language Models in Medicine. 43mn11.<br>
            [3] Zhang, Y., Backurs, A., Bubeck, S., Eldan, R., Gunasekar, S., & Wagner, T. (2022). Unveiling Transformers with LEGO: A synthetic reasoning task.<br>
            [4] Stanislas Dehaene. (1997). <em>La Bosse des maths</em>. Odile Jacob.<br>
            [5] Fletcher, J.M. (2005). Predicting math outcomes: reading predictors and comorbidity. <em>Journal of Learning Disabilities</em>.
          </p>
          <p class="subtle small">Illustration: <em>La Lecture</em>. (1892). Pierre-Auguste Renoir. Musée du Louvre.</p>
        `
      },
      { 
        title: "Les régressions technologiques", 
        img: "articles_images/article4.png", 
        text: `
          <p>En terminant cette semaine un projet et en rédigeant l’article qui en découle, je me questionnais sur la science qui avance toujours tambour battant, comme une locomotive, sans jamais regarder derrière elle. L’humanité ne sait pas désapprendre. Nous avons une grande difficulté à faire marche arrière, notamment en matière de technologie. Pourtant, à certaines périodes de l’histoire, nous avons volontairement abandonné des solutions ou simplement oublié un savoir-faire pour ne le redécouvrir que des siècles plus tard.</p>
      
          <p>L’exemple le plus connu est probablement le Concorde [1], mais en voici trois autres :</p>
      
          <p><strong>Les dirigeables</strong> — Une alternative prometteuse aux avions, notamment pour les longues distances. Silencieux, dotés d’une forte capacité de charge et moins polluants, ils étaient surnommés "paquebots aériens", et ce n'était pas exagéré, certains modèles atteignant plus de 240 mètres (le Titanic en faisait 269). Mais après la catastrophe du Hindenburg en 1937, l’aviation a pris le dessus et l’usage des dirigeables a quasiment disparu, malgré des avancées technologiques qui auraient permis de les sécuriser [2]. Comptez tout de même 4 jours et 15 heures pour un Francfort-Rio en 1930 : mieux vaut avoir le temps !</p>
      
          <p><strong>Le béton romain</strong> — Les Romains avaient mis au point un béton exceptionnellement durable, utilisé dans des ouvrages comme le Panthéon. Sa formule lui permettait de gagner en solidité avec le temps. Le secret de cette technologie résidait dans l’utilisation de la pouzzolane, une forme de cendre volcanique. Ce savoir-faire a été perdu après la chute de l’Empire romain, et il a fallu attendre le XIXe siècle et les avancées en chimie des matériaux pour retrouver une technologie s’approchant de son efficacité [3].</p>
      
          <p><strong>L’abandon des trains à grande vitesse aux États-Unis</strong> — Dans les années 1960, les États-Unis ont développé des trains à grande vitesse, comme le Metroliner. Mais, contrairement à l’Europe et au Japon, ils ont progressivement abandonné l’idée de développer un réseau ferroviaire à grande vitesse au profit du transport aérien et routier. Aujourd’hui, ce pays reste très en retard dans ce domaine.</p>
      
          <p>Je serais curieux d’en découvrir d’autres ! Vous en connaissez ?</p>
      
          <p class="subtle small">
            [1] Lawrence, P. (2015). <em>Concorde: The Rise and Fall of Supersonic Passenger Travel</em>.<br>
            [2] Grossman, D. (2012). <em>Zeppelins and the Age of Airships</em>.<br>
            [3] Jackson, M.D. (2014). Durability of Roman Concrete Structures. <em>Journal of the American Ceramic Society</em>.<br>
            Illustration : Pierre Saez, <em>Le Zeppelin</em>.
          </p>
        `
      },
      
      { 
        title: "Mon travail en stage", 
        img: "articles_images/article5.png", 
        text: `
          <p>Aujourd’hui, une grosse étape : notre équipe Brain & AI dévoile deux études [1,2 – preprints] auxquelles j’ai activement contribué. Un mois après le début de ma thèse, difficile d’imaginer meilleur timing pour renforcer ma motivation !</p>
      
          <p>En voici un résumé :</p>
      
          <p>Nous avons enregistré l’activité cérébrale de 35 participants pendant qu’ils tapaient des phrases, puis entraîné <em>Brain2Qwerty</em>, un modèle en trois parties (Convolutions, Transformer, Modèle de Langage), pour prédire leur saisie uniquement à partir de ces signaux cérébraux. Le modèle atteint une accuracy d’environ 80 % chez les meilleurs participants en magnétoencéphalographie.</p>
      
          <p>Ce n’est pas encore un outil utilisable au quotidien, mais c’est un net progrès par rapport aux approches actuelles basées sur l’EEG. Surtout, cela ouvre des perspectives prometteuses pour restaurer la communication chez des patients atteints de lésions cérébrales, et ce, sans nécessiter d’implants intracérébraux par chirurgie.</p>
      
          <p>Nos résultats suivent également les prédictions de la linguistique. Avant de taper un mot, le cerveau utilise une hiérarchie de représentation :</p>
      
          <ol>
            <li>Il active d’abord le contexte.</li>
            <li>Ensuite, la représentation du mot apparaît.</li>
            <li>Suivie de celle des syllabes.</li>
            <li>Enfin, chaque lettre est encodée et exécutée.</li>
          </ol>
      
          <p>Nous montrons qu’un code neuronal dynamique permet d’anticiper et d’enchaîner ces étapes de manière fluide, en représentant simultanément les frappes en cours et celles à venir.</p>
      
          <p>C'est un pas de plus vers la compréhension du langage dans le cerveau, une capacité clé de l’intelligence humaine.</p>
      
          <p>Plus de détails sur ce blog post de <em>AI at Meta</em> !</p>
      
          <p class="subtle small">
            [1] <em>Brain-to-Text Decoding: A Non-Invasive Approach via Typing</em> — <a href="https://lnkd.in/eRMHFNpZ" target="_blank">Lien</a><br>
            De : Jarod Lévy, Lucy (Mingfang) Zhang, Svetlana Pinet, Jérémy Rapin, Hubert Banville, Stéphane d'Ascoli*, Jean-Rémi King*<br><br>
            [2] <em>From Thought to Action: How a Hierarchy of Neural Dynamics Supports Language Production</em> — <a href="https://lnkd.in/eq96nH5u" target="_blank">Lien</a><br>
            De : Lucy (Mingfang) Zhang, Jarod Lévy, Stéphane d'Ascoli, Jérémy Rapin, F.-X. Alario, Pierre Bourdillon, Svetlana Pinet*, Jean-Rémi King*
          </p>
        `
      },
      
      { 
        title: "L’intelligence, les IAs et nous", 
        img: "articles_images/article6.png", 
        text: `
          <p>L'intelligence, selon son étymologie, désigne la capacité à "faire le lien" ou à "lire entre les lignes". J’aime beaucoup cette définition qui englobe l'essence de toutes les formes d'intelligence.</p>
      
          <p>Les systèmes dits "intelligents" excellent aujourd’hui dans des tâches spécialisées grâce à trois piliers fondamentaux : les réseaux de neurones, les cartes graphiques et l’accès massif aux données. L'entraînement des IA consomme des ressources énergétiques colossales, mobilisant des milliers de CPU et GPU, avec une consommation pouvant atteindre des centaines de kilowattheures. En comparaison, le cerveau humain fonctionne avec une énergie d’environ 20 watts. Cette efficacité biologique souligne la différence fondamentale entre une machine ultra-puissante et très gourmande, et l’intelligence humaine, sur-optimisée. Ce fossé ne sera sans doute jamais totalement comblé, mais nous pouvons tenter de le réduire, notamment en concevant des architectures inspirées de la biologie.</p>
      
          <p>Jean-Louis Dessalles, chercheur en intelligence artificielle, met en avant que l’étonnement est central à l’intelligence humaine [1]. C’est cette capacité à éveiller l’intérêt et à ajuster nos actions face aux imprévus qui distingue notre pensée. De nouvelles architectures comme les Titans [2] développées par Google, tentent de reproduire cette flexibilité cognitive.</p>
      
          <p>Il est d’autant plus urgent d’avoir des idées novatrices qu'une étude récente publiée dans <em>Nature</em> [3] (merci pour ton post Jean-Emmanuel Bibault) met en évidence un problème : nous approchons d'une pénurie de données pour entraîner les modèles. Selon cette recherche, d'ici 2028, la totalité du texte public disponible en ligne pourrait être exploitée.</p>
      
          <p>Face à ce défi, nous devons repenser nos approches. Plutôt que de poursuivre une course à l'entraînement massif, beaucoup se tournent vers la solution de développer des modèles hyper-spécialisés, conçus pour des tâches précises, en mode agent plutôt que généraliste. Si on suit cette piste-là, il est impératif que les dirigeants reformulent les métiers impactés afin de mieux les valoriser en les recentrant sur ce qui fait la richesse de l'intelligence humaine. Nous ne parlons pas assez de cet aspect.</p>
      
          <p>Et nous de notre côté ? Continuons à “faire le lien” et “lire entre les lignes”, encore et toujours, c’est ce pour quoi nous avons été programmés.</p>
      
          <p class="subtle small">
            [1] Rémy Demichelis (2019). <em>Pourquoi l'intelligence artificielle n'est pas intelligente?</em> Les Échos.<br>
            [2] Behrouz, A., Zhong, P., & Mirrokni, V. (2024). <em>Titans: Learning to memorize at test time</em>. arXiv preprint arXiv:2501.00663.<br>
            [3] Jones, N. (2024). <em>The AI revolution is running out of data. What can researchers do?</em> Nature, 636(8042), 290–292.<br>
            *L’illustration du post correspond à une des figures de cette étude.
          </p>
        `
      },
      { 
        title: "Être doctorant", 
        img: "articles_images/article7.png", 
        text: `
          <p>Parlons cette semaine pour la première fois depuis le début de cette série hebdomadaire de l’expérience de la thèse en elle-même.</p>
      
          <p>Pendant une thèse, on publie peu, on apprend beaucoup, et on passe notre temps à débugger. On se rend vite compte qu’un bon résultat est souvent un faux résultat. On est rapidement premier expert sur le petit projet sur lequel on travaille et donc vite seul face à lui. Le scepticisme est probablement la meilleure manière de ne pas se faire de fausses joies au début. C’est la « pratique délibérée » dont parle Ericsson [1]. Il montre qu’on s’engage sur un chemin où l’on sait que l’on fera de nombreuses erreurs, mais que la répétition et l’analyse des erreurs (partie souvent oubliée) permettent, au final, d’acquérir des compétences profondes.</p>
      
          <p>Le problème, c’est que lorsqu’une semaine passe sans aucun progrès, où tout bug, il est difficile de garder cela en tête. Les chiffres sont affolants. Une étude [2] montre que les doctorants sont six fois plus susceptibles de souffrir de dépression que la population générale. Les raisons principales seraient par exemple la pression pour publier, mais c’est bien l'isolement social qui arrive en tête de file. En même temps, qu’il est difficile, à une heure arbitraire de la soirée, de se dire que l’on a fini notre journée quand notre projet ne fonctionne toujours pas. On a aussi souvent le sentiment d’inutilité : à quoi va servir mon projet, qui a peu de chances de déboucher ?</p>
      
          <p>C’est pour cela qu’il est nécessaire de profiter du chemin sans être obsédé par les objectifs. Le débug d’un code, la lecture d’un bel article, un brouillon que l’on finit, doivent être des victoires quotidiennes. Il faut se rappeler sans cesse que nous sommes dans un environnement où l’on apprend tous les jours, en autonomie, auprès de personnes incroyables et passionnées, que l’on contribue humblement à l’avancée de la science.</p>
      
          <p>À nos dirigeants : ne nous faites pas regretter notre choix. La suppression du dispositif « Jeune Docteur » suite au vote du budget 2025 [3] est un exemple du peu de considération des autorités vis-à-vis des doctorants. Sans considération, sans poste ni rémunérations attractives, il y aura de moins en moins de doctorants en France aussi passionnés soient-ils. Les conséquences de telles décisions sont invisibles lorsqu’elles sont prises et il est déjà trop tard lorsque l’on s’en rend compte…</p>
      
          <p class="subtle small">
            [1] Ericsson, K. A., Krampe, R. T., & Tesch-Römer, C. (1993). <em>The Role of Deliberate Practice in the Acquisition of Expert Performance</em>. <em>Psychological Review</em>, 100(3).<br>
            [2] Evans, T. M., et al. (2018). Evidence for a mental health crisis in graduate education. <em>Nature Biotechnology</em>.<br>
            [3] Tribune, <em>Le Point</em>. (2025). La France se tire-t-elle une balle dans le pied en sacrifiant ses jeunes chercheurs ?<br><br>
          </p>
        `
      },
      
      { 
        title: "Prendre son temps", 
        img: "articles_images/article8.png", 
        text: `
          <p>L’âge moyen de la fin des études supérieures en France se situe entre 21,5 ans et 24 ans [1]. C’est très jeune comparé à d’autres pays comme la Suisse, où cet âge avoisine les 26 ans [2]. En France, arriver tôt sur le marché du travail après un parcours sans interruption est souvent perçu comme un symbole de réussite. Bien que de plus en plus de cursus proposent des années de césure, le processus d’ouverture à d’autres expériences reste inachevé.</p>
      
          <p>Dans certaines voies, notamment en médecine ou en classes préparatoires, on demande très tôt aux étudiants, à peine âgés de 18 ans, de s’enfermer dans un rythme de travail effréné. Sommes-nous suffisamment préparés, à cet âge-là, pour avoir la détermination nécessaire à un tel effort ? Ou bien sélectionne-t-on les étudiants non pas tant pour leurs performances et leur manière de réfléchir, mais plutôt pour leur capacité, à un âge démesurément jeune, à résister à la pression ?</p>
      
          <p>J’ai choisi d’étudier à l’étranger en partie pour cette raison : pour ne pas être sélectionné selon la résilience que j’ai à 18 ans. J’ai étudié dans trois pays différents et pris deux années pour découvrir le monde professionnel. Après ces détours, j’ai abordé avec beaucoup plus de sérénité et de détermination les démarches pour la suite de mon parcours. Sans ces expériences, je suis convaincu que je n’aurais jamais développé les capacités requises, et surtout, je n’aurais jamais attiré l’attention avec mon profil.</p>
      
          <p>Mais évidemment, j’ai eu la chance de faire ces choix uniquement grâce au niveau social de mes parents. 40 % des étudiants doivent travailler en parallèle de leurs études [2], on ne parle donc pas ici de retarder le moment où l’on commence à gagner sa vie. Mais lorsqu’il s’agit de construire une aspiration professionnelle durable — celle qui nous portera durant les 40 années de notre vie active —, il est essentiel de prendre son temps. D’après l’INJEP, plus d’un tiers des jeunes diplômés changent de voie dans les cinq premières années suivant leur entrée sur le marché du travail [3].</p>
      
          <p>Pour donner le temps à tout le monde, il faudrait repenser notre système de bourses et d’aides afin de ne pas simplement permettre de survivre, mais d’être réellement autonome, avec la place mentale nécessaire pour réfléchir à ses choix.</p>
      
          <p>« L’éducation n’est pas un vase que l’on remplit, mais un feu que l’on allume ». D’accord. Mais alors, permettons de l’alimenter par la curiosité, les rencontres et la liberté ultime : celle de prendre son temps pour faire le choix qui nous correspond.</p>
      
          <p class="subtle small">
            [1] DARES. (2021). Comment l’âge de sortie des études initiales s’articule-t-il avec le début de carrière ?<br>
            [2] OVE. (2019). Les étudiants en Europe : âges et parcours comparés. Info n°26.<br>
            [3] INJEP. (2020). Les reconversions précoces des jeunes diplômés : entre désillusion et quête de sens.<br><br>
          </p>
        `
      },
      
      { 
        title: "L’effet Matilda", 
        img: "articles_images/article9.png", 
        text: `
          <p>À l’occasion du 8 mars, je souhaitais parler du phénomène qui suit la science depuis ses débuts et qui a été désigné par l’“effet Matilda” [1]. C’est l’effet d’invisibilisation systémique des femmes dans la science. Cet effet ne se limite pas simplement à l'appropriation directe des travaux féminins par des chercheurs masculins. Il se manifeste également dans des situations de découvertes simultanées où le nom retenu par l’histoire est systématiquement celui du découvreur masculin. En voici deux exemples parmi les plus connus :</p>
      
          <p><strong>Lise Meitner</strong> — physicienne autrichienne, elle a été l’élément central de la découverte du principe de la fission nucléaire [2] en collaboration avec Otto Hahn, mais c’est seulement ce dernier qui a reçu seul le prix Nobel de chimie en 1944.</p>
      
          <p><strong>Jocelyn Bell Burnell</strong> — en 1967, elle est doctorante et observe, la première, des étoiles à neutrons très denses émettant des signaux radio périodiques : les pulsars [3]. C’est son directeur de thèse, Antony Hewish, qui reçut le prix Nobel en 1974, sans qu’elle ne soit mentionnée.</p>
      
          <p>Aujourd’hui, heureusement, ce phénomène est de plus en plus rare, les erreurs historiques ont été rectifiées et les chercheuses sont le plus souvent reconnues à leur juste titre. Mais il reste encore du chemin à parcourir pour l'égalité complète. Quelques chiffres : 2/3 des chercheurs sont des hommes dans le monde, seulement 28 % sont des femmes en France. Et ces chiffres chutent pour les postes à responsabilité. En Europe, seulement 11 % des postes de recherche seniors sont occupés par des femmes [4].</p>
      
          <p>J’aimerais ici remercier les femmes qui m’ont inspiré au fil de mon parcours professionnel. Et en citer trois qui ont changé mon parcours de manière significative.</p>
      
          <ul>
            <li><strong>Lena Bourhy</strong> – qui m’a transmis son amour pour les neurosciences lors de mon premier stage.</li>
            <li><strong>Clémentine Lévy-Fidel</strong> – avec qui j’ai quasiment découvert la programmation via un projet en première année à l’EPFL.</li>
            <li><strong>Claudia Clopath</strong> – qui ne le sait probablement pas, mais qui m’a donné un sublime cours de neurosciences computationnelles à Londres.</li>
          </ul>
      
          <p class="subtle small">
            [1] Matilda Joslyn Gage (1883). <em>Women as Inventor</em>.<br>
            [2] The Papers of Lise Meitner (1862–1979). Cambridge Archives.<br>
            [3] Josh Jones (2021). Open Culture. <em>Jocelyn Bell Burnell Changed Astronomy Forever; Her Ph.D. Advisor Won the Nobel Prize for It</em>.<br>
            [4] UNESCO (2021). <em>UNESCO Science Report: towards 2030</em>.<br>
            Illustrations : The New York Times (gauche), The New Yorker (droite).<br><br>        `
      },
      
      { 
        title: "Question pour un champion", 
        img: "articles_images/article10.png", 
        text: `
          <p>Cette semaine, un post un peu spécial : je suis passé à la télévision samedi dernier sur France 3 ! J’ai eu la chance de participer à l’émission <em>Question pour un champion</em> aux côtés de Samuel Étienne.</p>
      
          <p>Le replay est disponible ici : <a href="https://lnkd.in/erxPNnBA" target="_blank">Lien</a></p>
      
          <p>Un immense merci aux équipes de l’émission qui savent accueillir chaque candidat avec bienveillance, respect et empathie (et qui aident beaucoup à gérer le stress !). Merci aussi à Marie-Claire, Quentin et Kevin, d’excellents partenaires de jeu, avec qui j’ai passé un super moment.</p>
      
          <p>C’était génial de pouvoir participer à une émission qui existe depuis 1988 et de rencontrer au cours de cette journée autant de personnes passionnées par la culture générale.</p>
      
          <p>Je vous laisse découvrir l’émission pour voir jusqu’où je suis allé… 😉</p>
        `
      },
      { 
        title: "L’intelligence artificielle généralisée", 
        img: "articles_images/article11.png", 
        text: `
          <p>Deux visions s’opposent lorsqu’il est question de l’avenir de l’intelligence artificielle et des bénéfices qu’elle pourrait apporter. Dans ce post, je m’inspire largement des idées et des mots de ces deux camps.</p>
      
          <p>D’un côté, le camp de Dario Amodei, CEO d’Anthropic, qui a publié l’essai <em>Machines of Loving Grace</em> [1]. Il y évoque l’idée d’un XXI<sup>e</sup> siècle "compressé", dans lequel, après l’émergence d’une IA suffisamment puissante pour dépasser significativement l’intelligence humaine, les avancées que les chercheurs mettraient normalement 50 à 100 ans à accomplir pourraient être réalisées en 5 à 10 ans. Il parle de « milliers d’Einstein assis sur des datacenters ». Sam Altman, CEO d’OpenAI, partage cette vision (voir son tweet en illustration) : il pense que les IA finiront par remplacer totalement le travail intellectuel, ne laissant aux humains que les “métiers du monde physique”.</p>
      
          <p>De l’autre côté, Thomas Wolf, CSO de Hugging Face, propose une autre vision. Dans un de ses posts [2], il avance que les modèles actuels et futurs connaîtront peut-être toutes les réponses, mais pour faire une découverte significative comme, par exemple, celle de Copernic — où toutes les connaissances de son époque allaient à l’encontre de ses idées (en IA, on dirait “malgré son training set”) —, il faut autre chose que la simple accumulation de savoir. Pour faire avancer la science, il ne suffit pas de tout savoir. Il faut poser des questions que personne n’a encore imaginées.</p>
      
          <p>La technologie CRISPR [3], qui permet d’éditer les gènes et qui a valu un prix Nobel à ses découvreurs, est un bon exemple. Cette méthode repose sur des connaissances propres à un mécanisme bactérien identifié il y a plus de 20 ans. Il a pourtant fallu tout ce temps pour que quelqu’un ait l’intuition de l’appliquer à un usage radicalement nouveau.</p>
      
          <p>Savoir comment les modèles futurs pourront être capables de ce genre de saut créatif semble être la clé pour rejoindre le premier camp. Aujourd’hui, ces modèles ressemblent surtout à d’excellents élèves. Mais ce dont nous avons besoin, ce sont plutôt de bons chercheurs, capables de poser des questions inattendues.</p>
      
          <p>Pour l’instant et encore pour un moment, ces technologies auront besoin de nous pour être correctement orientées. Mais dans le futur, deux scénarios se dessinent : des avancées autonomes pilotées par ces modèles, ou une collaboration où l’humain reste l’étincelle principale de créativité. Alors, quel est votre avis ?</p>
      
          <p class="subtle small">
            [1] Dario Amodei. (2024). <em>Machines of Loving Grace</em>.<br>
            [2] Thomas Wolf. (2025). <em>The Einstein AI Model</em>.<br>
            [3] Jinek, M. et al. (2012). A programmable dual-RNA–guided DNA endonuclease in adaptive bacterial immunity.
          </p>
        `
      },
      
      { 
        title: "L’effet Dunning-Kruger", 
        img: "articles_images/article12.png", 
        text: `
          <p>Quand on débute dans un nouveau domaine, on est souvent confronté à un phénomène bien connu : le syndrome de l’imposteur. Ce sentiment de ne pas être légitime, de ne pas mériter sa place, malgré les preuves du contraire. Mais il existe un autre syndrome, moins évoqué et pourtant tout aussi fréquent : l’effet Dunning-Kruger.</p>
      
          <p>L’histoire de McArthur Wheeler l’illustre à merveille. En 1995, cet homme braque deux banques à visage découvert, convaincu d’être invisible grâce au jus de citron sur son visage. Il pensait que, comme pour l’encre invisible, cela le rendrait indétectable. Son incompréhension lors de son arrestation a inspiré les chercheurs Dunning et Kruger à théoriser un biais cognitif important [1] : les moins compétents sont souvent les plus confiants, car ils ignorent l’étendue de leur ignorance.</p>
      
          <p>On a donc un face-à-face entre les plus compétents qui doutent et les moins compétents qui surestiment leurs capacités. Ces deux effets coexistent souvent aux premières phases d’apprentissage. Le Dunning-Kruger intervient lorsqu'on ne sait pas ce qu’on ignore. Puis, en progressant, le syndrome de l’imposteur apparaît : on perçoit l’immensité du savoir à acquérir sans reconnaître le chemin déjà parcouru. Enfin, une fois compétent, on devient si sensible aux subtilités qu’on hésite à s’exprimer, de peur de simplifier à l’excès.</p>
      
          <p>D’autres études confirment ce schéma. En 2017, une équipe a montré que l’effet Dunning-Kruger se manifeste dans des tâches variées : logique, grammaire, humour [2]. Le syndrome de l’imposteur, quant à lui, est associé à des traits comme le perfectionnisme ou l’anxiété sociale [3]. Il faut tout de même nuancer : des chercheurs japonais [4] ont montré que ces effets varient selon les cultures. Ainsi, au Japon, on observe une tendance à la sous-évaluation, tandis que les participants américains de l’étude initiale avaient tendance à se surestimer.</p>
      
          <p>Dans le milieu professionnel et académique, les chercheurs s’accordent pour dire que ces deux effets cohabitent. Dans le débat public, on observe curieusement beaucoup moins le syndrome de l’imposteur et beaucoup plus d’exemples de l’effet Dunning-Kruger…</p>
      
          <p class="subtle small">
            [1] Kruger, J., & Dunning, D. (1999). <em>Unskilled and unaware of it: how difficulties in recognizing one's own incompetence lead to inflated self-assessments</em>.<br>
            [2] Pennycook, G., Ross, R. M., Koehler, D. J., & Fugelsang, J. A. (2017). <em>Dunning–Kruger effects in reasoning</em>.<br>
            [3] Bravata, D. M., et al. (2020). <em>Prevalence, predictors, and treatment of impostor syndrome: a systematic review</em>.<br>
            [4] Heine, S. J., et al. (2001). <em>Divergent consequences of success and failure in Japan and North America</em>.
          </p>
        `
      },
      
      { 
        title: "L’importance de l’ennui", 
        img: "articles_images/article13.png", 
        text: `
          <p>J’ai toujours lu que l’ennui était très important, autant dans les milieux de la psychologie positive que dans ceux de la productivité. Cette semaine, j’ai voulu en savoir plus en cherchant plusieurs études sur ce sujet. Je vous écris ici ce que j’en ai retenu.</p>
      
          <p>Quand on ne fait « rien », notre cerveau n’est pas inactif. Il entre dans un état particulier, celui du <em>Default Mode Network</em> [1]. Ce réseau est associé à la rêverie, à la projection dans le futur et aux souvenirs. Il correspond à toutes ces pensées spontanées qui émergent quand on ne suit aucune tâche dirigée. C’est littéralement le moment où l’on se « retrouve avec soi-même ».</p>
      
          <p>Une étude menée en 2013 [2] a demandé à des participants de recopier des numéros de téléphone pendant 15 minutes (une tâche ennuyeuse) avant de leur proposer un test de créativité : trouver un maximum d’usages possibles pour un gobelet en plastique. Le groupe qui devait réaliser la tâche ennuyeuse a généré significativement plus d’idées originales que celui du groupe contrôle. L’ennui déclenche une forme de pensée divergente, moins contrainte, plus exploratoire.</p>
      
          <p>Une autre étude de 2014 [3] a testé, cette fois-ci, différentes formes d’ennui : lecture de textes techniques, tâches répétitives, attente passive… Ils constatent que les tâches passives, celles qui laissent l’esprit vraiment libre, permettent encore plus la pensée créative que les tâches actives. Ces résultats permettent de vérifier l’intuition selon laquelle les meilleures idées n’arrivent pas souvent devant un fichier Word ou dans une réunion, mais surgissent plutôt dans des moments qui semblent aléatoires, non connectés, et où notre esprit est disponible.</p>
      
          <p>Évidemment, ces résultats sont d’autant plus importants durant l’enfance. Laisser les enfants s’ennuyer, c’est garantir une robustesse intérieure : on se supporte soi-même, et on s’invente intérieurement aussi.</p>
      
          <p>Je vous épargne donc le discours sur le fait que nos quotidiens ultra-connectés rendent difficiles ces moments d’ennui, mais après ces lectures, j’essaierai de divaguer un peu plus pour profiter de ces interstices, ces pauses mentales vides où se logent parfois des étincelles...</p>
      
          <p class="subtle small">
            [1] Raichle, M. E. (2015). <em>The brain's default mode network</em>. <em>Annual Review of Neuroscience</em>.<br>
            [2] Mann, S., & Cadman, R. (2013). <em>Does Being Bored Make Us More Creative?</em> <em>British Journal of Psychology</em>.<br>
            [3] Bench, S. W., & Lench, H. C. (2014). <em>On the Function of Boredom</em>. <em>Personality and Social Psychology Bulletin</em>.<br>
            Illustration : Joaquin Phoenix – <em>Her</em> (2013).
          </p>
        `
      },
      
      { 
        title: "Les Français et la science", 
        img: "articles_images/article14.png", 
        text: `
          <p>Le sondage <em>Science et Société</em> d’Ipsos et de l’Institut Sapiens [1], publié en octobre 2024, révèle des résultats contrastés sur le rapport des Français à la science. 51 % des personnes interrogées estiment que « ce n’est pas parce qu’un scientifique spécialisé sur un sujet me démontre un fait que c’est vrai et que ça vaut plus que mon jugement personnel ». Pourtant, 69 % pensent toujours que la science est essentielle pour répondre aux grands enjeux contemporains.</p>
      
          <p>La confiance envers les chercheurs reste élevée : 75 % pour le secteur public et 69 % pour le secteur privé. Cependant, des signaux préoccupants émergent. La proportion de Français estimant bien comprendre les résultats scientifiques est passée de 55 % à 50 %. Plus alarmant encore, 78 % des sondés trouvent de plus en plus difficile de distinguer les vraies informations scientifiques des fausses. Cette confusion alimente l’adhésion croissante à des idées infondées, comme la négation du rôle des humains dans le réchauffement climatique ou encore l’impact supposé de la 5G sur le système immunitaire.</p>
      
          <p>Le chiffre qui m’a particulièrement interpellé, c’est que plus de la moitié des Français affirment que leur jugement personnel prévaut, même face aux faits démontrés par un spécialiste. Cette méfiance provient en partie du doute concernant l’indépendance des scientifiques. Selon une étude réalisée par Cluster17 [2] pour <em>Le Point</em>, 56 % des Français estiment que les scientifiques ne sont pas indépendants et se laissent influencer par des groupes de pression. On assiste à l’émergence d’un scepticisme « éclairé » au sein des classes moyennes et des diplômés intermédiaires, particulièrement prudents face aux innovations scientifiques. Face à ces doutes, les Français préfèrent largement s’informer auprès de leurs proches (71 %), loin devant les médias (31 %, en baisse de 13 points) et le gouvernement (28 %). On voit dans chaque étude consultée [1,2,3] une attitude paradoxale envers la science, oscillant entre confiance et défiance.</p>
      
          <p>Pour éviter que la recherche ne devienne un monde hermétique réservé aux initiés, il semble primordial d'insister dès l’école sur les principes du processus scientifique. En tant que doctorant, je pense qu’il est nécessaire de consacrer un temps substantiel à rendre accessible son travail pour qu’il ait le plus d’impact possible, tant dans le monde scientifique que dans les consciences de la population. C’est ce travail qui préservera la vérité scientifique en France et évitera les dérives observées ailleurs, notamment aux États-Unis.</p>
      
          <p class="subtle small">
            [1] Ipsos, Institut Sapiens (2022, 2024). <em>Baromètre Science et Société</em>.<br>
            [2] Cluster17 (2025). <em>Les Français et la science : un rapport marqué par des fractures et des paradoxes</em>.<br>
            [3] CNRS Journal (2022). <em>Les Français et la science : une relation ambivalente</em>.
          </p>
        `
      },
      
      { 
        title: "Je parle régulièrement à un canard", 
        img: "articles_images/article15.png", 
        text: `
          <p>Cette semaine on parle de cette recherche qui fait 𝗩𝗜𝗕𝗥𝗘𝗥, qui nous émoustille et des techniques un peu bizarres mais efficaces pour trouver ses bugs.</p>
      
          <p>Ça fait trois mois que j’ai commencé ma thèse. Le début était intense : on finalisait la publication de mon travail de stage (<a href="https://lnkd.in/emDgcjmz" target="_blank">à lire ici</a>). Depuis, le rythme a changé. Je suis passé à une phase plus floue, plus exploratoire. Beaucoup de tests. Beaucoup d’hypothèses. Et… beaucoup de bugs.</p>
      
          <p>Pour ceux qui, comme moi, ne connaissaient pas encore (merci Jérémy Rapin 💡), voici la meilleure stratégie qu’on donne pour débugger un code : le canard en plastique 🦆. Tu expliques ton bug à un canard posé sur ton bureau. Le simple fait de formuler à voix haute suffit, dans la majorité des cas, à en débloquer la cause.</p>
      
          <p>Mais parfois – heureusement – il y a des moments où tu lances un script et ça marche. Pas forcément parfaitement mais juste assez pour que quelque chose se passe. Une courbe qui change. Une métrique qui grimpe. Une intuition qui se confirme.</p>
      
          <p>Ce moment d’excitation, justifie à lui seul l’envie de faire de la recherche. Un moment de calme et de satisfaction. Au milieu du chaos des notebooks, des checkpoints et des logs : un court instant où on a le sentiment d’avoir compris quelque chose. Les chercheurs n’ont rien à envier aux métiers les plus palpitants. La recherche fait vibrer !</p>
      
          <p>Pendant une thèse, aller voir les résultats d’un script lancé la veille, c’est un vrai moment palpitant. D’autant plus qu’en tant qu’étudiant, on ne comprend pas encore tout donc on est souvent un peu trop excité : chaque résultat paraît potentiellement révolutionnaire. (Alors qu’il est souvent soit déjà bien connu depuis 20 ans… soit juste dû à un bug). On est peut-être un peu trop excités — mais c’est aussi ça qui est beau.</p>
      
          <p>Les chercheurs plus expérimentés semblent s’assagir avec le temps. Certains semblent résignés mais beaucoup gardent cette conviction que 𝗾𝘂𝗲𝗹𝗾𝘂𝗲 𝗰𝗵𝗼𝘀𝗲 𝗱𝗲 𝗳𝗼𝗿𝗺𝗶𝗱𝗮𝗯𝗹𝗲 𝗲𝘀𝘁 𝗽𝗼𝘀𝘀𝗶𝗯𝗹𝗲 — je pense qu’il faut 𝗰𝗵𝗲́𝗿𝗶𝗿 cette conviction. La garder vivante aussi longtemps que possible.</p>
      
          <p>Et vous ? C'est quoi votre moment préféré en recherche ?</p>
        `
      },
      { 
        title: "Conference survival guide for a PhD student", 
        img: "articles_images/article16.png", 
        text: `
          <p>This week, I am in Singapore at the <em>#ICLR2025</em> conference.</p>
      
          <p>It was my first big AI conference, and I wanted to share some <strong>tips and tricks</strong> that older PhD students and my supervisors shared with me — super useful for newcomers like me. Attending a conference of this size is intense: hundreds of posters, talks everywhere, people everywhere. At first, it feels impossible to know what to do or who to talk to.</p>
      
          <p><strong>1 — Have a plan!</strong><br>
          Before the conference, list the posters you really want to see and the people you'd love to meet. Often, these large conferences offer nice visualizations to help you find interesting papers in your area. For <em>#ICLR</em>, Hendrik Strobelt and Benjamin Hoover [1] created a really helpful visualization tool for exploring the projects.</p>
      
          <p><strong>2 — Don’t feel guilty if you’re not coding or writing.</strong><br>
          It’s useless to attend a conference if you are constantly thinking about your ongoing projects. Of course, sometimes we have deadlines, but conferences are about opening doors and discovering knowledge you didn’t even know existed. It’s messy, but valuable. You decided to attend — so enjoy it as much as possible.</p>
      
          <p><strong>3 — Prepare for the full zoo of conference characters 🎭</strong><br>
          The Hater: who thinks your project should have never existed, and maybe that you shouldn’t even be doing research at all.<br>
          The Rockstar: the famous researcher everyone knows, always surrounded by a little crowd.<br>
          The Social Butterfly: doesn’t really care much about posters or talks, but knows everyone and gets invited to all the networking events.<br>
          The Exhausted PhD: attends every single session, collects 50 PDFs to read later (never will), and takes notes on every poster they've seen.<br>
          The Recruit: secretly (?) trying to charm and get hired by your lab. Be kind to them. Remember, at some point, you will be this character too.<br>
          The Curious One: the kind of researcher you want to become — curious, open, respectful, and genuinely excited about science.</p>
      
          <p><strong>Final advice 🌟🌟</strong><br>
          Everyone is here to discuss, so don’t be afraid to say hi! Especially to people you admire.</p>
      
          <p>If you're heading to a conference soon, I hope this will help a little. It helped me a lot. ✌️</p>
      
          <p class="subtle small">
            [1] Strobelt, H. & Hoover, B. (2022). <em>Interactive Corpora Visualization for 60 Years of AI Research</em>. Proceedings of the NeurIPS 2021 Competitions and Demonstrations Track.
          </p>
        `
      },
      
      { 
        title: "The art of communication in research", 
        img: "articles_images/article17.png", 
        text: `
          <p>This week, I’ve been reflecting on a delicate tension: <strong>how to make a research project visible without distorting its meaning</strong>.</p>
      
          <p>Every week, I post a snapshot of what makes me think, question, or smile during my PhD journey. Starting this week, these reflections will also be posted on Substack. Subscribe <a href="https://lnkd.in/eDEzWuti" target="_blank">here</a> to receive them directly in your inbox.</p>
      
          <p>In research, doing the work isn’t enough. You also need to show that it was done and make its value legible to others. Without visibility, even meaningful contributions risk being forgotten. But, in trying to be understood, it’s easy to overstate. Somewhere between silence and hype lies honest communication — and that’s not always easy to find. People have very limited time to give to your study, and if it receives media coverage, the message will likely be oversimplified and amplified to reach the broadest audience.</p>
      
          <p>This situation came into sharp focus recently when my team published a new paper [1]. The topic — neuroscience — and the affiliation — Meta — drew immediate media attention. But soon, some articles misrepresented our work: claiming we had achieved “mind reading,” when in fact we had decoded typed sentences from participants under very specific, controlled conditions. But once a message leaves your hands, people see what they want to see.</p>
      
          <p><strong>The lesson?</strong> Say what you did. Say what it means. Say what it doesn’t mean. And then stop.</p>
      
          <p><strong>Some practical rules for honest communication I try to follow:</strong><br>
          1/ One paper = one strong, memorable idea. Make your Figure 1 count.<br>
          2/ Be ready to pitch even unfinished work — early feedback sharpens your vision.<br>
          3/ Leave breadcrumbs, not confetti. Let your path speak clearly over time.</p>
      
          <p>Maxwell Forbes [2] reframes the research paper not just as a report, but also as an advertisement:<br>
          1/ A currency (for conferences, grants, and jobs)<br>
          2/ A brand-builder (shaping how you're perceived)<br>
          3/ An ad (selling one idea to readers with limited time)</p>
      
          <p>It’s a powerful lens — but a dangerous one too. The temptation to polish the image beyond the substance is real. And yet, the responsibility remains: we owe readers clarity without distortion, and we owe ourselves the discipline of restraint. An easy way to overcome these issues? <strong>Reproducibility</strong>, of course.</p>
      
          <p class="subtle small">
            [1] Lévy et al. (2025). <em>Brain-to-Text Decoding: A Non-invasive Approach via Typing</em>.<br>
            [2] Maxwell Forbes (2025). <em>The PhD Metagame: Your Paper is an Ad</em>.
          </p>
        `
      },
      
      { 
        title: "Neuroscience is weird (and that’s why we love it)", 
        img: "articles_images/article18.png", 
        text: `
          <p>This week, I took some time off and revisited a few neuroscience studies that are as surprising as they are insightful. Some even won Ig Nobel prizes: awards for research that “first makes you laugh, then makes you think.”</p>
      
          <p><strong>1/ Seeing faces everywhere is a survival mechanism [1]</strong><br>
          That face you spot in a tree trunk, on your morning tartine, or in the clouds? It’s pareidolia. Your brain is wired to detect faces, even when there is little chance it’s a real one. More recent studies [5] suggest men are more prone to it than women. Why? From an evolutionary perspective, a false alarm is less costly than missing a real threat.</p>
      
          <p><strong>2/ Swearing increases pain tolerance [2]</strong><br>
          In this study, participants were asked to keep their hands in ice water for as long as they could. They lasted significantly longer when allowed to swear. But only real swear words worked — not neutral words or fake ones like “fouch.” Swearing activates emotional brain circuits and reduces pain perception. It seems that letting it out really does help.</p>
      
          <p><strong>3/ Chocolate tastes better with the right music [3]</strong><br>
          Soft jazz can make chocolate taste sweeter. Harsh sounds make it taste more bitter. This is called crossmodal correspondence: your brain blends sound and taste. OK for music but it goes beyond that — flavour isn’t just on the tongue. Smell, vision, touch — they all contribute. Maybe that’s why your grandma’s recipe never tastes quite the same today? Not the same smell in the house, not the same light, not the same sounds. Perception is a full-body experience.</p>
      
          <p><strong>4/ “Huh?” is a universal word [4]</strong><br>
          Researchers found that the sound “huh?” exists in every language. It’s a universal repair signal. It’s your brain’s way of saying: “Wait, something’s off.” Like a shared human error 404.</p>
      
          <p>All four of these studies remind me of the same thing: the brain isn’t a perfect logic machine. It’s a living, improvising system. It was built for meaning, not truth.</p>
      
          <p>Would love to hear if you’ve come across neuroscience studies that made you laugh or rethink something!</p>
      
          <p class="subtle small">
            [1] Liu et al. (2014). <em>Seeing Jesus in toast: Neural and behavioral correlates of face pareidolia</em>.<br>
            [2] Stephens et al. (2020). <em>Swearing as a Response to Pain</em>.<br>
            [3] Crisinel et al. (2012). <em>A bittersweet symphony: Systematically modulating the taste of food by changing the sonic properties of the soundtrack playing in the background</em>.<br>
            [4] Dingemanse et al. (2013). <em>Is “Huh?” a universal word?</em> <em>Conversational Infrastructure and the Convergent Evolution of Linguistic Items</em>.<br>
            [5] Wardle et al. (2022). <em>Illusory faces are more likely to be perceived as male than female</em>.<br>
            (Illustration)
          </p>
        `
      },
      
      { 
        title: "Rethinking how we hire researchers", 
        img: "articles_images/article19.png", 
        text: `
          <p>AI researchers today write less code and produce more experiments. That’s not laziness. It’s a shift in the job itself.</p>
      
          <p>In AI research, and more broadly, in any research area that involves coding, the landscape has changed. Just a few years ago, testing one idea a day felt productive. Now, running through five or even ten variants in a single afternoon isn’t unusual. That’s thanks to our tight integration with LLMs. You sketch an idea, describe it, and the AI handles the boilerplate, the syntax, and even the silent bugs. Exit the wasted afternoons caused by a missing comma. Prompt well and you’ve got working code in minutes.</p>
      
          <p>You probably know this already. But while our workflows have evolved, recruiting hasn’t always caught up. So, here’s a question: should we still prioritize technical depth when hiring, or focus more on scientific thinking and problem-solving?</p>
      
          <p>Understanding what you’re doing matters. But beyond that, the real differentiators are probably how well you think, how creatively you approach problems, and a newcomer: how fast you can iterate. I can see it every day, the most effective researchers aren’t necessarily the most “technical,” but they’re sharp, fast, and precise.</p>
      
          <p>Yet many of us are still shy with LLMs. We open ChatGPT in a side tab. We hide the window when someone walks by. A recent study [1] even showed that using LLMs at work is perceived as a sign of weakness. But if you really understand your task, and code is the tool, then LLMs can enhance your productivity tenfold, especially when you’re a junior.</p>
      
          <p>That’s why we need to assess people based on how they actually work. Is it still relevant to make candidates memorize hundreds of Leetcode problems they’ll never use again? Thankfully, it’s already partially reflected in current pipelines. AI Research Design Interviews, for example, are a step in the right direction, asking candidates to explore real research questions. But the pure coding component might need an update.</p>
      
          <p>It reminds me of how exams changed during COVID: when everyone had internet access, memorization didn’t matter as much anymore. Understanding did.</p>
      
          <p>Every week, I share something that makes me think, smile, or question things during my PhD journey. These reflections are now on Substack too. You can subscribe <a href="https://lnkd.in/eDEzWuti" target="_blank">here</a> to receive them directly in your inbox.</p>
      
          <p class="subtle small">
            [1] Restrepo Amariles, D. (2025). <em>Usage caché de ChatGPT</em>.<br>
            [2] Preston, D. (2025). <em>Trump’s new tariff math looks a lot like ChatGPT’s</em>. The Verge.<br>
            Illustration : <em>The Internship</em> (2013).
          </p>
        `
      },
      
      { 
        title: "Discipline or quality?", 
        img: "articles_images/article20.png", 
        text: `
          <p>After 20 weeks of weekly posts, I wanted to say thank you. Writing these posts and connecting with many of you has been both energizing and meaningful. It's been a pleasant surprise to see that there is room for this kind of content on LinkedIn.</p>
      
          <p>I’ll be moving to a monthly long-form format on Substack, with shorter summaries shared here. This change is about finding a better rhythm allowing to write with more depth and clarity. Between LinkedIn's character limits and the time constraints of a PhD, the weekly format has started to feel limiting.</p>
      
          <p>Beyond the value of this PhD logbook, I still believe it’s possible to share ideas that are nuanced, well-sourced, and grounded in reality. Even at a time when those values feel increasingly rare. That’s what I’ll keep aiming for, just at a slower and more intentional pace.</p>
      
          <p>Thanks again for following along. I hope to see you over there.<br>
          Substack: <a href="https://lnkd.in/eDEzWuti" target="_blank">https://lnkd.in/eDEzWuti</a></p>
        `
      },
  ];