const articles = [
    {   title: "La nuance ne fragilise pas le propos",
            img: "articles_images/article1.png",
            text: `
            <p>En 1945, dans <em>The Crack-Up</em>, F. Scott Fitzgerald Ã©crit : 
            Â« La marque dâ€™une intelligence de premier ordre est la capacitÃ© de tenir simultanÃ©ment deux idÃ©es opposÃ©es dans son esprit tout en conservant la capacitÃ© de fonctionner. Â»</p>

            <p>ÃŠtre doctorant confÃ¨re un statut un peu particulier : sans Ãªtre reconnu comme un expert, nous sommes pourtant en premiÃ¨re ligne. 
            Le mÃ©tier de chercheur a cela dâ€™original que lâ€™on passe une majoritÃ© de notre temps Ã  explorer des hypothÃ¨ses fausses. 
            Pourtant il est de plus en plus dur de dire ouvertement â€œje ne sais pasâ€. On craint de ne plus Ãªtre pris au sÃ©rieux. 
            Cela impacte l'ensemble du monde de la recherche. Trop de publications positives, pas assez de publications nÃ©gatives. 
            Rester disciplinÃ© dans ses recherches, ne pas cÃ©der Ã  la tentation dâ€™enjoliver ses rÃ©sultats et concÃ©der parfois que lâ€™on ne sait simplement pas est fondamental, 
            comme lâ€™explique le journaliste Alexandre Morales dans son Ã©mission du 25 dÃ©cembre sur France Culture [1]. 
            Câ€™est le premier rempart Ã  la dÃ©sinformation. Car si la recherche triche, qui ne trichera pas ?</p>

            <p>Ã€ l'extÃ©rieur du monde de la recherche, la sociÃ©tÃ© semble avoir changÃ© de configuration. 
            On encourage maintenant la prise de position tranchÃ©e, le cru, l'extrÃªme. 
            Le public est-il encore Ã  la recherche de la vÃ©ritÃ© ? 
            On sait dÃ©jÃ  quâ€™il est beaucoup plus facile de rÃ©pandre une fausse information quâ€™une vraie, 
            comme lâ€™a dÃ©montrÃ© cet excellent article du MIT Media Lab [2].</p>

            <p>Dâ€™un point de vue cÃ©rÃ©bral, comment le cerveau apprÃ©hende-t-il le fait que la quantitÃ© dâ€™information qui nous parvient a brutalement changÃ© dâ€™ordre de grandeur ? 
            Je me questionne. Combien de temps me suis-je accordÃ© pour avoir un avis sur le nouveau gouvernement syrien ? 
            Et sur le nouveau Premier ministre franÃ§ais ? Pourquoi devrions-nous avoir un avis sur tout ? 
            Il semble parfois qu'Ãªtre informÃ© sans pour autant tout connaÃ®tre soit devenu impensable. 
            Et changer de point de vue ? Nâ€™en parlons mÃªme pas !</p>

            <p>Je reprends ici largement les propos d'Ã‰tienne Klein, IsmaÃ«l Khelifa et d'autres qui prÃ´nent une nuance immodÃ©rÃ©e.</p>

            <p>Cela semble Ã©vident, câ€™est vrai. Selon la loi de Brandolini [3], 
            "la quantitÃ© d'Ã©nergie nÃ©cessaire pour rÃ©futer des sottises [...] est supÃ©rieure d'un ordre de grandeur Ã  celle nÃ©cessaire pour les produire."</p>

            <p>C'est fatigant, c'est vrai, mais ne laissons pas l'entiÃ¨retÃ© de la sphÃ¨re publique aux positionnements schÃ©matiques.</p>

            <p class="subtle small">
            [1] Alexandre Morales â€“ France Culture (2024).<br>
            [2] Vosoughi, S., Roy, D., & Aral, S. (2018). The spread of true and false news online. <em>Science</em>, 359(6380), 1146â€“1151.<br>
            [3] Bullshit Asymmetry Principle â€“ Alberto Brandolini (2013).
            
            
            <p>        </p>
            
            
            </p>   `
      },
      {
        title: "L'apprentissage des langues ne doit pas Ãªtre menacÃ©",
        img: "articles_images/article2.png",
        text: `
          <p>Ã€ NoÃ«l, je discutais avec ma tante, professeure dâ€™italien. Elle mâ€™a fait part de ses inquiÃ©tudes concernant les avancÃ©es de lâ€™IA et leurs rÃ©percussions sur lâ€™apprentissage des langues.</p>
          <p>Apprendre une langue, est-ce dÃ©jÃ  une pratique dÃ©passÃ©e ? Si, dans le futur, on peut dÃ©coder en direct ce que lâ€™autre nous dit, Ã  quoi bon passer des annÃ©es Ã  apprendre une langue ?</p>
          <p>Pourtant, supprimer cet apprentissage sous prÃ©texte quâ€™on peut avoir accÃ¨s Ã  une traduction immÃ©diate est, pour beaucoup de neuroscientifiques, passer Ã  cÃ´tÃ© de lâ€™essentiel. Lâ€™objectif principal peut Ãªtre de parler couramment, mais le processus dâ€™apprentissage transforme surtout durablement le cerveau, mÃªme si la personne est incapable, aujourdâ€™hui, dâ€™aligner trois mots dans cette langue.</p>
          <p>Raphael Gaillard, psychiatre et acadÃ©micien, en parle dans son livre <em>Lâ€™Homme AugmentÃ©</em> [1]. Il Ã©crit : Â« La culture ne vaut pas tant par ce que nous pouvons en restituer en termes de connaissances, mais par ce quâ€™il en reste une fois que nous avons tout oubliÃ©. Â» Cette idÃ©e est assez ancienne, on la trouve dÃ©jÃ  Ã  la fin du XIXáµ‰ siÃ¨cle avec Ellen Key, pÃ©dagogue suÃ©doise [2]. Ce que nous avons su un jour laisse une empreinte sur nous. Chaque moment passÃ© Ã  tenter dâ€™apprendre quelque chose modifie notre rÃ©seau neuronal interne, et notre cerveau, bien quâ€™il puisse oublier, ne fonctionnera plus jamais comme avant cet apprentissage.</p>
          <p>Lâ€™important nâ€™est donc pas de retenir, mais bien dâ€™apprendre. Dâ€™autant plus quâ€™il existe maintenant un quasi-consensus sur le fait que les personnes qui ne cessent jamais dâ€™apprendre ralentissent leur dÃ©clin cognitif et sont moins touchÃ©es par des maladies neurodÃ©gÃ©nÃ©ratives comme Alzheimer [3]. Il ne faut jamais cesser d'apprendre.</p>
          <p>Dans un futur proche, les IA seront donc probablement utilisÃ©es massivement comme traducteurs instantanÃ©s. Cela ne doit cependant pas rimer avec lâ€™abandon de lâ€™apprentissage des langues. Car apprendre une langue, câ€™est se donner la chance de rÃ©flÃ©chir dans un autre systÃ¨me. Câ€™est Ã©galement se donner la possibilitÃ© de penser comme lâ€™autre, celui qui parle cette langue. Câ€™est, avant tout, un acte social.</p>
          <p class="subtle small">
            [1] Raphael Gaillard. (2024). <em>Lâ€™Homme AugmentÃ©</em>. Grasset.<br>
            [2] Ellen Key. (1891). <em>On tue lâ€™esprit dans les Ã©coles</em>. Revue Verdandi.<br>
            [3] Loef, M., & Walach, H. (2012). The combined effects of healthy lifestyle behaviors on all-cause mortality: A systematic review and meta-analysis. <em>Preventive Medicine</em>.
          </p>
        `
      },
      {
        title: "Les vertus de la lecture",
        img: "articles_images/article3.png",
        text: `
          <p>Cette semaine, je souhaitais partager une des Ã©tudes qui mâ€™a le plus impressionnÃ© ces derniÃ¨res annÃ©es et qui montre encore une fois l'intÃ©rÃªt d'Ã©tudier la frontiÃ¨re entre neurosciences et intelligence artificielle. Jâ€™ai dÃ©couvert ce rÃ©sultat en lisant <em>Lâ€™Homme AugmentÃ©</em> du neuropsychiatre RaphaÃ«l Gaillard [1] (remarquable livre !).</p>
          <p>Dans un podcast [2] du New England Journal of Medicine avec Peter Lee, VP research chez Microsoft, celui-ci mentionne un papier de SÃ©bastien Bubeck [3]. Cet article rapporte une expÃ©rience oÃ¹ des chercheurs ont entraÃ®nÃ© un rÃ©seau de neurones pour rÃ©soudre des systÃ¨mes d'Ã©quations linÃ©aires Ã  trois inconnues. Sans surprise, aprÃ¨s lâ€™entraÃ®nement, le modÃ¨le rÃ©ussit Ã  rÃ©soudre ce type de systÃ¨me, mais Ã©choue face Ã  des systÃ¨mes Ã  quatre inconnues. Cependant, lorsque lâ€™entraÃ®nement est rÃ©pÃ©tÃ© en ajoutant aux donnÃ©es un corpus volumineux de textes non mathÃ©matiques, le modÃ¨le devient alors capable de rÃ©soudre nâ€™importe quel systÃ¨me d'Ã©quations linÃ©aires.</p>
          <p>RaphaÃ«l Gaillard commente ainsi : Â« Ces IA gÃ©nÃ©ratives, comme ChatGPT, nous montrent ce qui rend intelligent : la lecture. Â» Et Ã  propos de lâ€™article de Bubeck, il ajoute : Â« Le simple fait de lire permet au modÃ¨le dâ€™atteindre un niveau de complexitÃ© supÃ©rieur en mathÃ©matiques. Â»</p>
          <p>Chez lâ€™enfant, plusieurs Ã©tudes, notamment les travaux de Stanislas Dehaene [4], ont Ã©galement dÃ©montrÃ© que les performances en lecture sont corrÃ©lÃ©es aux performances en mathÃ©matiques [5]. Il semblerait que de la mÃªme maniÃ¨re que le rÃ©seau de neurones artificiels, la lecture peut changer la conformation du cerveau pour mieux apprÃ©hender les mathÃ©matiques.</p>
          <p>Bref, tout cela donne envie de lire ! Voici trois rÃ¨gles quâ€™un grand lecteur mâ€™a confiÃ©es, que jâ€™applique depuis le dÃ©but de lâ€™annÃ©e, et qui fonctionnent vraiment bien pour lire rÃ©guliÃ¨rement (avant cela, je ne finissais presque aucun livreâ€¦) :</p>
          <ol>
            <li>Toujours avoir en tÃªte le prochain livre quâ€™on a envie de lire pour ne pas perdre la dynamique.</li>
            <li>Si, aprÃ¨s 50 pages, la lecture reste un effort, passer Ã  un autre livre.</li>
            <li>Ã€ moins quâ€™il ne sâ€™agisse dâ€™une recommandation ou dâ€™un chef-dâ€™Å“uvre. Dans ce cas-lÃ , on essaie encore ou on le met de cÃ´tÃ© pour plus tard.</li>
          </ol>
          <p class="subtle small">
            [1] RaphaÃ«l Gaillard. (2024). <em>Lâ€™Homme AugmentÃ©</em>. Grasset. Page 314.<br>
            [2] NEJM Group (2024) - Microsoftâ€™s Peter Lee on the Future of Language Models in Medicine. 43mn11.<br>
            [3] Zhang, Y., Backurs, A., Bubeck, S., Eldan, R., Gunasekar, S., & Wagner, T. (2022). Unveiling Transformers with LEGO: A synthetic reasoning task.<br>
            [4] Stanislas Dehaene. (1997). <em>La Bosse des maths</em>. Odile Jacob.<br>
            [5] Fletcher, J.M. (2005). Predicting math outcomes: reading predictors and comorbidity. <em>Journal of Learning Disabilities</em>.
          </p>
          <p class="subtle small">Illustration: <em>La Lecture</em>. (1892). Pierre-Auguste Renoir. MusÃ©e du Louvre.</p>
        `
      },
      { 
        title: "Les rÃ©gressions technologiques", 
        img: "articles_images/article4.png", 
        text: `
          <p>En terminant cette semaine un projet et en rÃ©digeant lâ€™article qui en dÃ©coule, je me questionnais sur la science qui avance toujours tambour battant, comme une locomotive, sans jamais regarder derriÃ¨re elle. Lâ€™humanitÃ© ne sait pas dÃ©sapprendre. Nous avons une grande difficultÃ© Ã  faire marche arriÃ¨re, notamment en matiÃ¨re de technologie. Pourtant, Ã  certaines pÃ©riodes de lâ€™histoire, nous avons volontairement abandonnÃ© des solutions ou simplement oubliÃ© un savoir-faire pour ne le redÃ©couvrir que des siÃ¨cles plus tard.</p>
      
          <p>Lâ€™exemple le plus connu est probablement le Concorde [1], mais en voici trois autres :</p>
      
          <p><strong>Les dirigeables</strong> â€” Une alternative prometteuse aux avions, notamment pour les longues distances. Silencieux, dotÃ©s dâ€™une forte capacitÃ© de charge et moins polluants, ils Ã©taient surnommÃ©s "paquebots aÃ©riens", et ce n'Ã©tait pas exagÃ©rÃ©, certains modÃ¨les atteignant plus de 240 mÃ¨tres (le Titanic en faisait 269). Mais aprÃ¨s la catastrophe du Hindenburg en 1937, lâ€™aviation a pris le dessus et lâ€™usage des dirigeables a quasiment disparu, malgrÃ© des avancÃ©es technologiques qui auraient permis de les sÃ©curiser [2]. Comptez tout de mÃªme 4 jours et 15 heures pour un Francfort-Rio en 1930 : mieux vaut avoir le temps !</p>
      
          <p><strong>Le bÃ©ton romain</strong> â€” Les Romains avaient mis au point un bÃ©ton exceptionnellement durable, utilisÃ© dans des ouvrages comme le PanthÃ©on. Sa formule lui permettait de gagner en soliditÃ© avec le temps. Le secret de cette technologie rÃ©sidait dans lâ€™utilisation de la pouzzolane, une forme de cendre volcanique. Ce savoir-faire a Ã©tÃ© perdu aprÃ¨s la chute de lâ€™Empire romain, et il a fallu attendre le XIXe siÃ¨cle et les avancÃ©es en chimie des matÃ©riaux pour retrouver une technologie sâ€™approchant de son efficacitÃ© [3].</p>
      
          <p><strong>Lâ€™abandon des trains Ã  grande vitesse aux Ã‰tats-Unis</strong> â€” Dans les annÃ©es 1960, les Ã‰tats-Unis ont dÃ©veloppÃ© des trains Ã  grande vitesse, comme le Metroliner. Mais, contrairement Ã  lâ€™Europe et au Japon, ils ont progressivement abandonnÃ© lâ€™idÃ©e de dÃ©velopper un rÃ©seau ferroviaire Ã  grande vitesse au profit du transport aÃ©rien et routier. Aujourdâ€™hui, ce pays reste trÃ¨s en retard dans ce domaine.</p>
      
          <p>Je serais curieux dâ€™en dÃ©couvrir dâ€™autres ! Vous en connaissez ?</p>
      
          <p class="subtle small">
            [1] Lawrence, P. (2015). <em>Concorde: The Rise and Fall of Supersonic Passenger Travel</em>.<br>
            [2] Grossman, D. (2012). <em>Zeppelins and the Age of Airships</em>.<br>
            [3] Jackson, M.D. (2014). Durability of Roman Concrete Structures. <em>Journal of the American Ceramic Society</em>.<br>
            Illustration : Pierre Saez, <em>Le Zeppelin</em>.
          </p>
        `
      },
      
      { 
        title: "Mon travail en stage", 
        img: "articles_images/article5.png", 
        text: `
          <p>Aujourdâ€™hui, une grosse Ã©tape : notre Ã©quipe Brain & AI dÃ©voile deux Ã©tudes [1,2 â€“ preprints] auxquelles jâ€™ai activement contribuÃ©. Un mois aprÃ¨s le dÃ©but de ma thÃ¨se, difficile dâ€™imaginer meilleur timing pour renforcer ma motivation !</p>
      
          <p>En voici un rÃ©sumÃ© :</p>
      
          <p>Nous avons enregistrÃ© lâ€™activitÃ© cÃ©rÃ©brale de 35 participants pendant quâ€™ils tapaient des phrases, puis entraÃ®nÃ© <em>Brain2Qwerty</em>, un modÃ¨le en trois parties (Convolutions, Transformer, ModÃ¨le de Langage), pour prÃ©dire leur saisie uniquement Ã  partir de ces signaux cÃ©rÃ©braux. Le modÃ¨le atteint une accuracy dâ€™environ 80 % chez les meilleurs participants en magnÃ©toencÃ©phalographie.</p>
      
          <p>Ce nâ€™est pas encore un outil utilisable au quotidien, mais câ€™est un net progrÃ¨s par rapport aux approches actuelles basÃ©es sur lâ€™EEG. Surtout, cela ouvre des perspectives prometteuses pour restaurer la communication chez des patients atteints de lÃ©sions cÃ©rÃ©brales, et ce, sans nÃ©cessiter dâ€™implants intracÃ©rÃ©braux par chirurgie.</p>
      
          <p>Nos rÃ©sultats suivent Ã©galement les prÃ©dictions de la linguistique. Avant de taper un mot, le cerveau utilise une hiÃ©rarchie de reprÃ©sentation :</p>
      
          <ol>
            <li>Il active dâ€™abord le contexte.</li>
            <li>Ensuite, la reprÃ©sentation du mot apparaÃ®t.</li>
            <li>Suivie de celle des syllabes.</li>
            <li>Enfin, chaque lettre est encodÃ©e et exÃ©cutÃ©e.</li>
          </ol>
      
          <p>Nous montrons quâ€™un code neuronal dynamique permet dâ€™anticiper et dâ€™enchaÃ®ner ces Ã©tapes de maniÃ¨re fluide, en reprÃ©sentant simultanÃ©ment les frappes en cours et celles Ã  venir.</p>
      
          <p>C'est un pas de plus vers la comprÃ©hension du langage dans le cerveau, une capacitÃ© clÃ© de lâ€™intelligence humaine.</p>
      
          <p>Plus de dÃ©tails sur ce blog post de <em>AI at Meta</em> !</p>
      
          <p class="subtle small">
            [1] <em>Brain-to-Text Decoding: A Non-Invasive Approach via Typing</em> â€” <a href="https://lnkd.in/eRMHFNpZ" target="_blank">Lien</a><br>
            De : Jarod LÃ©vy, Lucy (Mingfang) Zhang, Svetlana Pinet, JÃ©rÃ©my Rapin, Hubert Banville, StÃ©phane d'Ascoli*, Jean-RÃ©mi King*<br><br>
            [2] <em>From Thought to Action: How a Hierarchy of Neural Dynamics Supports Language Production</em> â€” <a href="https://lnkd.in/eq96nH5u" target="_blank">Lien</a><br>
            De : Lucy (Mingfang) Zhang, Jarod LÃ©vy, StÃ©phane d'Ascoli, JÃ©rÃ©my Rapin, F.-X. Alario, Pierre Bourdillon, Svetlana Pinet*, Jean-RÃ©mi King*
          </p>
        `
      },
      
      { 
        title: "Lâ€™intelligence, les IAs et nous", 
        img: "articles_images/article6.png", 
        text: `
          <p>L'intelligence, selon son Ã©tymologie, dÃ©signe la capacitÃ© Ã  "faire le lien" ou Ã  "lire entre les lignes". Jâ€™aime beaucoup cette dÃ©finition qui englobe l'essence de toutes les formes d'intelligence.</p>
      
          <p>Les systÃ¨mes dits "intelligents" excellent aujourdâ€™hui dans des tÃ¢ches spÃ©cialisÃ©es grÃ¢ce Ã  trois piliers fondamentaux : les rÃ©seaux de neurones, les cartes graphiques et lâ€™accÃ¨s massif aux donnÃ©es. L'entraÃ®nement des IA consomme des ressources Ã©nergÃ©tiques colossales, mobilisant des milliers de CPU et GPU, avec une consommation pouvant atteindre des centaines de kilowattheures. En comparaison, le cerveau humain fonctionne avec une Ã©nergie dâ€™environ 20 watts. Cette efficacitÃ© biologique souligne la diffÃ©rence fondamentale entre une machine ultra-puissante et trÃ¨s gourmande, et lâ€™intelligence humaine, sur-optimisÃ©e. Ce fossÃ© ne sera sans doute jamais totalement comblÃ©, mais nous pouvons tenter de le rÃ©duire, notamment en concevant des architectures inspirÃ©es de la biologie.</p>
      
          <p>Jean-Louis Dessalles, chercheur en intelligence artificielle, met en avant que lâ€™Ã©tonnement est central Ã  lâ€™intelligence humaine [1]. Câ€™est cette capacitÃ© Ã  Ã©veiller lâ€™intÃ©rÃªt et Ã  ajuster nos actions face aux imprÃ©vus qui distingue notre pensÃ©e. De nouvelles architectures comme les Titans [2] dÃ©veloppÃ©es par Google, tentent de reproduire cette flexibilitÃ© cognitive.</p>
      
          <p>Il est dâ€™autant plus urgent dâ€™avoir des idÃ©es novatrices qu'une Ã©tude rÃ©cente publiÃ©e dans <em>Nature</em> [3] (merci pour ton post Jean-Emmanuel Bibault) met en Ã©vidence un problÃ¨me : nous approchons d'une pÃ©nurie de donnÃ©es pour entraÃ®ner les modÃ¨les. Selon cette recherche, d'ici 2028, la totalitÃ© du texte public disponible en ligne pourrait Ãªtre exploitÃ©e.</p>
      
          <p>Face Ã  ce dÃ©fi, nous devons repenser nos approches. PlutÃ´t que de poursuivre une course Ã  l'entraÃ®nement massif, beaucoup se tournent vers la solution de dÃ©velopper des modÃ¨les hyper-spÃ©cialisÃ©s, conÃ§us pour des tÃ¢ches prÃ©cises, en mode agent plutÃ´t que gÃ©nÃ©raliste. Si on suit cette piste-lÃ , il est impÃ©ratif que les dirigeants reformulent les mÃ©tiers impactÃ©s afin de mieux les valoriser en les recentrant sur ce qui fait la richesse de l'intelligence humaine. Nous ne parlons pas assez de cet aspect.</p>
      
          <p>Et nous de notre cÃ´tÃ© ? Continuons Ã  â€œfaire le lienâ€ et â€œlire entre les lignesâ€, encore et toujours, câ€™est ce pour quoi nous avons Ã©tÃ© programmÃ©s.</p>
      
          <p class="subtle small">
            [1] RÃ©my Demichelis (2019). <em>Pourquoi l'intelligence artificielle n'est pas intelligente?</em> Les Ã‰chos.<br>
            [2] Behrouz, A., Zhong, P., & Mirrokni, V. (2024). <em>Titans: Learning to memorize at test time</em>. arXiv preprint arXiv:2501.00663.<br>
            [3] Jones, N. (2024). <em>The AI revolution is running out of data. What can researchers do?</em> Nature, 636(8042), 290â€“292.<br>
            *Lâ€™illustration du post correspond Ã  une des figures de cette Ã©tude.
          </p>
        `
      },
      { 
        title: "ÃŠtre doctorant", 
        img: "articles_images/article7.png", 
        text: `
          <p>Parlons cette semaine pour la premiÃ¨re fois depuis le dÃ©but de cette sÃ©rie hebdomadaire de lâ€™expÃ©rience de la thÃ¨se en elle-mÃªme.</p>
      
          <p>Pendant une thÃ¨se, on publie peu, on apprend beaucoup, et on passe notre temps Ã  dÃ©bugger. On se rend vite compte quâ€™un bon rÃ©sultat est souvent un faux rÃ©sultat. On est rapidement premier expert sur le petit projet sur lequel on travaille et donc vite seul face Ã  lui. Le scepticisme est probablement la meilleure maniÃ¨re de ne pas se faire de fausses joies au dÃ©but. Câ€™est la Â« pratique dÃ©libÃ©rÃ©e Â» dont parle Ericsson [1]. Il montre quâ€™on sâ€™engage sur un chemin oÃ¹ lâ€™on sait que lâ€™on fera de nombreuses erreurs, mais que la rÃ©pÃ©tition et lâ€™analyse des erreurs (partie souvent oubliÃ©e) permettent, au final, dâ€™acquÃ©rir des compÃ©tences profondes.</p>
      
          <p>Le problÃ¨me, câ€™est que lorsquâ€™une semaine passe sans aucun progrÃ¨s, oÃ¹ tout bug, il est difficile de garder cela en tÃªte. Les chiffres sont affolants. Une Ã©tude [2] montre que les doctorants sont six fois plus susceptibles de souffrir de dÃ©pression que la population gÃ©nÃ©rale. Les raisons principales seraient par exemple la pression pour publier, mais câ€™est bien l'isolement social qui arrive en tÃªte de file. En mÃªme temps, quâ€™il est difficile, Ã  une heure arbitraire de la soirÃ©e, de se dire que lâ€™on a fini notre journÃ©e quand notre projet ne fonctionne toujours pas. On a aussi souvent le sentiment dâ€™inutilitÃ© : Ã  quoi va servir mon projet, qui a peu de chances de dÃ©boucher ?</p>
      
          <p>Câ€™est pour cela quâ€™il est nÃ©cessaire de profiter du chemin sans Ãªtre obsÃ©dÃ© par les objectifs. Le dÃ©bug dâ€™un code, la lecture dâ€™un bel article, un brouillon que lâ€™on finit, doivent Ãªtre des victoires quotidiennes. Il faut se rappeler sans cesse que nous sommes dans un environnement oÃ¹ lâ€™on apprend tous les jours, en autonomie, auprÃ¨s de personnes incroyables et passionnÃ©es, que lâ€™on contribue humblement Ã  lâ€™avancÃ©e de la science.</p>
      
          <p>Ã€ nos dirigeants : ne nous faites pas regretter notre choix. La suppression du dispositif Â« Jeune Docteur Â» suite au vote du budget 2025 [3] est un exemple du peu de considÃ©ration des autoritÃ©s vis-Ã -vis des doctorants. Sans considÃ©ration, sans poste ni rÃ©munÃ©rations attractives, il y aura de moins en moins de doctorants en France aussi passionnÃ©s soient-ils. Les consÃ©quences de telles dÃ©cisions sont invisibles lorsquâ€™elles sont prises et il est dÃ©jÃ  trop tard lorsque lâ€™on sâ€™en rend compteâ€¦</p>
      
          <p class="subtle small">
            [1] Ericsson, K. A., Krampe, R. T., & Tesch-RÃ¶mer, C. (1993). <em>The Role of Deliberate Practice in the Acquisition of Expert Performance</em>. <em>Psychological Review</em>, 100(3).<br>
            [2] Evans, T. M., et al. (2018). Evidence for a mental health crisis in graduate education. <em>Nature Biotechnology</em>.<br>
            [3] Tribune, <em>Le Point</em>. (2025). La France se tire-t-elle une balle dans le pied en sacrifiant ses jeunes chercheurs ?<br><br>
          </p>
        `
      },
      
      { 
        title: "Prendre son temps", 
        img: "articles_images/article8.png", 
        text: `
          <p>Lâ€™Ã¢ge moyen de la fin des Ã©tudes supÃ©rieures en France se situe entre 21,5 ans et 24 ans [1]. Câ€™est trÃ¨s jeune comparÃ© Ã  dâ€™autres pays comme la Suisse, oÃ¹ cet Ã¢ge avoisine les 26 ans [2]. En France, arriver tÃ´t sur le marchÃ© du travail aprÃ¨s un parcours sans interruption est souvent perÃ§u comme un symbole de rÃ©ussite. Bien que de plus en plus de cursus proposent des annÃ©es de cÃ©sure, le processus dâ€™ouverture Ã  dâ€™autres expÃ©riences reste inachevÃ©.</p>
      
          <p>Dans certaines voies, notamment en mÃ©decine ou en classes prÃ©paratoires, on demande trÃ¨s tÃ´t aux Ã©tudiants, Ã  peine Ã¢gÃ©s de 18 ans, de sâ€™enfermer dans un rythme de travail effrÃ©nÃ©. Sommes-nous suffisamment prÃ©parÃ©s, Ã  cet Ã¢ge-lÃ , pour avoir la dÃ©termination nÃ©cessaire Ã  un tel effort ? Ou bien sÃ©lectionne-t-on les Ã©tudiants non pas tant pour leurs performances et leur maniÃ¨re de rÃ©flÃ©chir, mais plutÃ´t pour leur capacitÃ©, Ã  un Ã¢ge dÃ©mesurÃ©ment jeune, Ã  rÃ©sister Ã  la pression ?</p>
      
          <p>Jâ€™ai choisi dâ€™Ã©tudier Ã  lâ€™Ã©tranger en partie pour cette raison : pour ne pas Ãªtre sÃ©lectionnÃ© selon la rÃ©silience que jâ€™ai Ã  18 ans. Jâ€™ai Ã©tudiÃ© dans trois pays diffÃ©rents et pris deux annÃ©es pour dÃ©couvrir le monde professionnel. AprÃ¨s ces dÃ©tours, jâ€™ai abordÃ© avec beaucoup plus de sÃ©rÃ©nitÃ© et de dÃ©termination les dÃ©marches pour la suite de mon parcours. Sans ces expÃ©riences, je suis convaincu que je nâ€™aurais jamais dÃ©veloppÃ© les capacitÃ©s requises, et surtout, je nâ€™aurais jamais attirÃ© lâ€™attention avec mon profil.</p>
      
          <p>Mais Ã©videmment, jâ€™ai eu la chance de faire ces choix uniquement grÃ¢ce au niveau social de mes parents. 40 % des Ã©tudiants doivent travailler en parallÃ¨le de leurs Ã©tudes [2], on ne parle donc pas ici de retarder le moment oÃ¹ lâ€™on commence Ã  gagner sa vie. Mais lorsquâ€™il sâ€™agit de construire une aspiration professionnelle durable â€” celle qui nous portera durant les 40 annÃ©es de notre vie active â€”, il est essentiel de prendre son temps. Dâ€™aprÃ¨s lâ€™INJEP, plus dâ€™un tiers des jeunes diplÃ´mÃ©s changent de voie dans les cinq premiÃ¨res annÃ©es suivant leur entrÃ©e sur le marchÃ© du travail [3].</p>
      
          <p>Pour donner le temps Ã  tout le monde, il faudrait repenser notre systÃ¨me de bourses et dâ€™aides afin de ne pas simplement permettre de survivre, mais dâ€™Ãªtre rÃ©ellement autonome, avec la place mentale nÃ©cessaire pour rÃ©flÃ©chir Ã  ses choix.</p>
      
          <p>Â« Lâ€™Ã©ducation nâ€™est pas un vase que lâ€™on remplit, mais un feu que lâ€™on allume Â». Dâ€™accord. Mais alors, permettons de lâ€™alimenter par la curiositÃ©, les rencontres et la libertÃ© ultime : celle de prendre son temps pour faire le choix qui nous correspond.</p>
      
          <p class="subtle small">
            [1] DARES. (2021). Comment lâ€™Ã¢ge de sortie des Ã©tudes initiales sâ€™articule-t-il avec le dÃ©but de carriÃ¨re ?<br>
            [2] OVE. (2019). Les Ã©tudiants en Europe : Ã¢ges et parcours comparÃ©s. Info nÂ°26.<br>
            [3] INJEP. (2020). Les reconversions prÃ©coces des jeunes diplÃ´mÃ©s : entre dÃ©sillusion et quÃªte de sens.<br><br>
          </p>
        `
      },
      
      { 
        title: "Lâ€™effet Matilda", 
        img: "articles_images/article9.png", 
        text: `
          <p>Ã€ lâ€™occasion du 8 mars, je souhaitais parler du phÃ©nomÃ¨ne qui suit la science depuis ses dÃ©buts et qui a Ã©tÃ© dÃ©signÃ© par lâ€™â€œeffet Matildaâ€ [1]. Câ€™est lâ€™effet dâ€™invisibilisation systÃ©mique des femmes dans la science. Cet effet ne se limite pas simplement Ã  l'appropriation directe des travaux fÃ©minins par des chercheurs masculins. Il se manifeste Ã©galement dans des situations de dÃ©couvertes simultanÃ©es oÃ¹ le nom retenu par lâ€™histoire est systÃ©matiquement celui du dÃ©couvreur masculin. En voici deux exemples parmi les plus connus :</p>
      
          <p><strong>Lise Meitner</strong> â€” physicienne autrichienne, elle a Ã©tÃ© lâ€™Ã©lÃ©ment central de la dÃ©couverte du principe de la fission nuclÃ©aire [2] en collaboration avec Otto Hahn, mais câ€™est seulement ce dernier qui a reÃ§u seul le prix Nobel de chimie en 1944.</p>
      
          <p><strong>Jocelyn Bell Burnell</strong> â€” en 1967, elle est doctorante et observe, la premiÃ¨re, des Ã©toiles Ã  neutrons trÃ¨s denses Ã©mettant des signaux radio pÃ©riodiques : les pulsars [3]. Câ€™est son directeur de thÃ¨se, Antony Hewish, qui reÃ§ut le prix Nobel en 1974, sans quâ€™elle ne soit mentionnÃ©e.</p>
      
          <p>Aujourdâ€™hui, heureusement, ce phÃ©nomÃ¨ne est de plus en plus rare, les erreurs historiques ont Ã©tÃ© rectifiÃ©es et les chercheuses sont le plus souvent reconnues Ã  leur juste titre. Mais il reste encore du chemin Ã  parcourir pour l'Ã©galitÃ© complÃ¨te. Quelques chiffres : 2/3 des chercheurs sont des hommes dans le monde, seulement 28 % sont des femmes en France. Et ces chiffres chutent pour les postes Ã  responsabilitÃ©. En Europe, seulement 11 % des postes de recherche seniors sont occupÃ©s par des femmes [4].</p>
      
          <p>Jâ€™aimerais ici remercier les femmes qui mâ€™ont inspirÃ© au fil de mon parcours professionnel. Et en citer trois qui ont changÃ© mon parcours de maniÃ¨re significative.</p>
      
          <ul>
            <li><strong>Lena Bourhy</strong> â€“ qui mâ€™a transmis son amour pour les neurosciences lors de mon premier stage.</li>
            <li><strong>ClÃ©mentine LÃ©vy-Fidel</strong> â€“ avec qui jâ€™ai quasiment dÃ©couvert la programmation via un projet en premiÃ¨re annÃ©e Ã  lâ€™EPFL.</li>
            <li><strong>Claudia Clopath</strong> â€“ qui ne le sait probablement pas, mais qui mâ€™a donnÃ© un sublime cours de neurosciences computationnelles Ã  Londres.</li>
          </ul>
      
          <p class="subtle small">
            [1] Matilda Joslyn Gage (1883). <em>Women as Inventor</em>.<br>
            [2] The Papers of Lise Meitner (1862â€“1979). Cambridge Archives.<br>
            [3] Josh Jones (2021). Open Culture. <em>Jocelyn Bell Burnell Changed Astronomy Forever; Her Ph.D. Advisor Won the Nobel Prize for It</em>.<br>
            [4] UNESCO (2021). <em>UNESCO Science Report: towards 2030</em>.<br>
            Illustrations : The New York Times (gauche), The New Yorker (droite).<br><br>        `
      },
      
      { 
        title: "Question pour un champion", 
        img: "articles_images/article10.png", 
        text: `
          <p>Cette semaine, un post un peu spÃ©cial : je suis passÃ© Ã  la tÃ©lÃ©vision samedi dernier sur France 3 ! Jâ€™ai eu la chance de participer Ã  lâ€™Ã©mission <em>Question pour un champion</em> aux cÃ´tÃ©s de Samuel Ã‰tienne.</p>
      
          <p>Le replay est disponible ici : <a href="https://lnkd.in/erxPNnBA" target="_blank">Lien</a></p>
      
          <p>Un immense merci aux Ã©quipes de lâ€™Ã©mission qui savent accueillir chaque candidat avec bienveillance, respect et empathie (et qui aident beaucoup Ã  gÃ©rer le stress !). Merci aussi Ã  Marie-Claire, Quentin et Kevin, dâ€™excellents partenaires de jeu, avec qui jâ€™ai passÃ© un super moment.</p>
      
          <p>Câ€™Ã©tait gÃ©nial de pouvoir participer Ã  une Ã©mission qui existe depuis 1988 et de rencontrer au cours de cette journÃ©e autant de personnes passionnÃ©es par la culture gÃ©nÃ©rale.</p>
      
          <p>Je vous laisse dÃ©couvrir lâ€™Ã©mission pour voir jusquâ€™oÃ¹ je suis allÃ©â€¦ ğŸ˜‰</p>
        `
      },
      { 
        title: "Lâ€™intelligence artificielle gÃ©nÃ©ralisÃ©e", 
        img: "articles_images/article11.png", 
        text: `
          <p>Deux visions sâ€™opposent lorsquâ€™il est question de lâ€™avenir de lâ€™intelligence artificielle et des bÃ©nÃ©fices quâ€™elle pourrait apporter. Dans ce post, je mâ€™inspire largement des idÃ©es et des mots de ces deux camps.</p>
      
          <p>Dâ€™un cÃ´tÃ©, le camp de Dario Amodei, CEO dâ€™Anthropic, qui a publiÃ© lâ€™essai <em>Machines of Loving Grace</em> [1]. Il y Ã©voque lâ€™idÃ©e dâ€™un XXI<sup>e</sup> siÃ¨cle "compressÃ©", dans lequel, aprÃ¨s lâ€™Ã©mergence dâ€™une IA suffisamment puissante pour dÃ©passer significativement lâ€™intelligence humaine, les avancÃ©es que les chercheurs mettraient normalement 50 Ã  100 ans Ã  accomplir pourraient Ãªtre rÃ©alisÃ©es en 5 Ã  10 ans. Il parle de Â« milliers dâ€™Einstein assis sur des datacenters Â». Sam Altman, CEO dâ€™OpenAI, partage cette vision (voir son tweet en illustration) : il pense que les IA finiront par remplacer totalement le travail intellectuel, ne laissant aux humains que les â€œmÃ©tiers du monde physiqueâ€.</p>
      
          <p>De lâ€™autre cÃ´tÃ©, Thomas Wolf, CSO de Hugging Face, propose une autre vision. Dans un de ses posts [2], il avance que les modÃ¨les actuels et futurs connaÃ®tront peut-Ãªtre toutes les rÃ©ponses, mais pour faire une dÃ©couverte significative comme, par exemple, celle de Copernic â€” oÃ¹ toutes les connaissances de son Ã©poque allaient Ã  lâ€™encontre de ses idÃ©es (en IA, on dirait â€œmalgrÃ© son training setâ€) â€”, il faut autre chose que la simple accumulation de savoir. Pour faire avancer la science, il ne suffit pas de tout savoir. Il faut poser des questions que personne nâ€™a encore imaginÃ©es.</p>
      
          <p>La technologie CRISPR [3], qui permet dâ€™Ã©diter les gÃ¨nes et qui a valu un prix Nobel Ã  ses dÃ©couvreurs, est un bon exemple. Cette mÃ©thode repose sur des connaissances propres Ã  un mÃ©canisme bactÃ©rien identifiÃ© il y a plus de 20 ans. Il a pourtant fallu tout ce temps pour que quelquâ€™un ait lâ€™intuition de lâ€™appliquer Ã  un usage radicalement nouveau.</p>
      
          <p>Savoir comment les modÃ¨les futurs pourront Ãªtre capables de ce genre de saut crÃ©atif semble Ãªtre la clÃ© pour rejoindre le premier camp. Aujourdâ€™hui, ces modÃ¨les ressemblent surtout Ã  dâ€™excellents Ã©lÃ¨ves. Mais ce dont nous avons besoin, ce sont plutÃ´t de bons chercheurs, capables de poser des questions inattendues.</p>
      
          <p>Pour lâ€™instant et encore pour un moment, ces technologies auront besoin de nous pour Ãªtre correctement orientÃ©es. Mais dans le futur, deux scÃ©narios se dessinent : des avancÃ©es autonomes pilotÃ©es par ces modÃ¨les, ou une collaboration oÃ¹ lâ€™humain reste lâ€™Ã©tincelle principale de crÃ©ativitÃ©. Alors, quel est votre avis ?</p>
      
          <p class="subtle small">
            [1] Dario Amodei. (2024). <em>Machines of Loving Grace</em>.<br>
            [2] Thomas Wolf. (2025). <em>The Einstein AI Model</em>.<br>
            [3] Jinek, M. et al. (2012). A programmable dual-RNAâ€“guided DNA endonuclease in adaptive bacterial immunity.
          </p>
        `
      },
      
      { 
        title: "Lâ€™effet Dunning-Kruger", 
        img: "articles_images/article12.png", 
        text: `
          <p>Quand on dÃ©bute dans un nouveau domaine, on est souvent confrontÃ© Ã  un phÃ©nomÃ¨ne bien connu : le syndrome de lâ€™imposteur. Ce sentiment de ne pas Ãªtre lÃ©gitime, de ne pas mÃ©riter sa place, malgrÃ© les preuves du contraire. Mais il existe un autre syndrome, moins Ã©voquÃ© et pourtant tout aussi frÃ©quent : lâ€™effet Dunning-Kruger.</p>
      
          <p>Lâ€™histoire de McArthur Wheeler lâ€™illustre Ã  merveille. En 1995, cet homme braque deux banques Ã  visage dÃ©couvert, convaincu dâ€™Ãªtre invisible grÃ¢ce au jus de citron sur son visage. Il pensait que, comme pour lâ€™encre invisible, cela le rendrait indÃ©tectable. Son incomprÃ©hension lors de son arrestation a inspirÃ© les chercheurs Dunning et Kruger Ã  thÃ©oriser un biais cognitif important [1] : les moins compÃ©tents sont souvent les plus confiants, car ils ignorent lâ€™Ã©tendue de leur ignorance.</p>
      
          <p>On a donc un face-Ã -face entre les plus compÃ©tents qui doutent et les moins compÃ©tents qui surestiment leurs capacitÃ©s. Ces deux effets coexistent souvent aux premiÃ¨res phases dâ€™apprentissage. Le Dunning-Kruger intervient lorsqu'on ne sait pas ce quâ€™on ignore. Puis, en progressant, le syndrome de lâ€™imposteur apparaÃ®t : on perÃ§oit lâ€™immensitÃ© du savoir Ã  acquÃ©rir sans reconnaÃ®tre le chemin dÃ©jÃ  parcouru. Enfin, une fois compÃ©tent, on devient si sensible aux subtilitÃ©s quâ€™on hÃ©site Ã  sâ€™exprimer, de peur de simplifier Ã  lâ€™excÃ¨s.</p>
      
          <p>Dâ€™autres Ã©tudes confirment ce schÃ©ma. En 2017, une Ã©quipe a montrÃ© que lâ€™effet Dunning-Kruger se manifeste dans des tÃ¢ches variÃ©es : logique, grammaire, humour [2]. Le syndrome de lâ€™imposteur, quant Ã  lui, est associÃ© Ã  des traits comme le perfectionnisme ou lâ€™anxiÃ©tÃ© sociale [3]. Il faut tout de mÃªme nuancer : des chercheurs japonais [4] ont montrÃ© que ces effets varient selon les cultures. Ainsi, au Japon, on observe une tendance Ã  la sous-Ã©valuation, tandis que les participants amÃ©ricains de lâ€™Ã©tude initiale avaient tendance Ã  se surestimer.</p>
      
          <p>Dans le milieu professionnel et acadÃ©mique, les chercheurs sâ€™accordent pour dire que ces deux effets cohabitent. Dans le dÃ©bat public, on observe curieusement beaucoup moins le syndrome de lâ€™imposteur et beaucoup plus dâ€™exemples de lâ€™effet Dunning-Krugerâ€¦</p>
      
          <p class="subtle small">
            [1] Kruger, J., & Dunning, D. (1999). <em>Unskilled and unaware of it: how difficulties in recognizing one's own incompetence lead to inflated self-assessments</em>.<br>
            [2] Pennycook, G., Ross, R. M., Koehler, D. J., & Fugelsang, J. A. (2017). <em>Dunningâ€“Kruger effects in reasoning</em>.<br>
            [3] Bravata, D. M., et al. (2020). <em>Prevalence, predictors, and treatment of impostor syndrome: a systematic review</em>.<br>
            [4] Heine, S. J., et al. (2001). <em>Divergent consequences of success and failure in Japan and North America</em>.
          </p>
        `
      },
      
      { 
        title: "Lâ€™importance de lâ€™ennui", 
        img: "articles_images/article13.png", 
        text: `
          <p>Jâ€™ai toujours lu que lâ€™ennui Ã©tait trÃ¨s important, autant dans les milieux de la psychologie positive que dans ceux de la productivitÃ©. Cette semaine, jâ€™ai voulu en savoir plus en cherchant plusieurs Ã©tudes sur ce sujet. Je vous Ã©cris ici ce que jâ€™en ai retenu.</p>
      
          <p>Quand on ne fait Â« rien Â», notre cerveau nâ€™est pas inactif. Il entre dans un Ã©tat particulier, celui du <em>Default Mode Network</em> [1]. Ce rÃ©seau est associÃ© Ã  la rÃªverie, Ã  la projection dans le futur et aux souvenirs. Il correspond Ã  toutes ces pensÃ©es spontanÃ©es qui Ã©mergent quand on ne suit aucune tÃ¢che dirigÃ©e. Câ€™est littÃ©ralement le moment oÃ¹ lâ€™on se Â« retrouve avec soi-mÃªme Â».</p>
      
          <p>Une Ã©tude menÃ©e en 2013 [2] a demandÃ© Ã  des participants de recopier des numÃ©ros de tÃ©lÃ©phone pendant 15 minutes (une tÃ¢che ennuyeuse) avant de leur proposer un test de crÃ©ativitÃ© : trouver un maximum dâ€™usages possibles pour un gobelet en plastique. Le groupe qui devait rÃ©aliser la tÃ¢che ennuyeuse a gÃ©nÃ©rÃ© significativement plus dâ€™idÃ©es originales que celui du groupe contrÃ´le. Lâ€™ennui dÃ©clenche une forme de pensÃ©e divergente, moins contrainte, plus exploratoire.</p>
      
          <p>Une autre Ã©tude de 2014 [3] a testÃ©, cette fois-ci, diffÃ©rentes formes dâ€™ennui : lecture de textes techniques, tÃ¢ches rÃ©pÃ©titives, attente passiveâ€¦ Ils constatent que les tÃ¢ches passives, celles qui laissent lâ€™esprit vraiment libre, permettent encore plus la pensÃ©e crÃ©ative que les tÃ¢ches actives. Ces rÃ©sultats permettent de vÃ©rifier lâ€™intuition selon laquelle les meilleures idÃ©es nâ€™arrivent pas souvent devant un fichier Word ou dans une rÃ©union, mais surgissent plutÃ´t dans des moments qui semblent alÃ©atoires, non connectÃ©s, et oÃ¹ notre esprit est disponible.</p>
      
          <p>Ã‰videmment, ces rÃ©sultats sont dâ€™autant plus importants durant lâ€™enfance. Laisser les enfants sâ€™ennuyer, câ€™est garantir une robustesse intÃ©rieure : on se supporte soi-mÃªme, et on sâ€™invente intÃ©rieurement aussi.</p>
      
          <p>Je vous Ã©pargne donc le discours sur le fait que nos quotidiens ultra-connectÃ©s rendent difficiles ces moments dâ€™ennui, mais aprÃ¨s ces lectures, jâ€™essaierai de divaguer un peu plus pour profiter de ces interstices, ces pauses mentales vides oÃ¹ se logent parfois des Ã©tincelles...</p>
      
          <p class="subtle small">
            [1] Raichle, M. E. (2015). <em>The brain's default mode network</em>. <em>Annual Review of Neuroscience</em>.<br>
            [2] Mann, S., & Cadman, R. (2013). <em>Does Being Bored Make Us More Creative?</em> <em>British Journal of Psychology</em>.<br>
            [3] Bench, S. W., & Lench, H. C. (2014). <em>On the Function of Boredom</em>. <em>Personality and Social Psychology Bulletin</em>.<br>
            Illustration : Joaquin Phoenix â€“ <em>Her</em> (2013).
          </p>
        `
      },
      
      { 
        title: "Les FranÃ§ais et la science", 
        img: "articles_images/article14.png", 
        text: `
          <p>Le sondage <em>Science et SociÃ©tÃ©</em> dâ€™Ipsos et de lâ€™Institut Sapiens [1], publiÃ© en octobre 2024, rÃ©vÃ¨le des rÃ©sultats contrastÃ©s sur le rapport des FranÃ§ais Ã  la science. 51 % des personnes interrogÃ©es estiment que Â« ce nâ€™est pas parce quâ€™un scientifique spÃ©cialisÃ© sur un sujet me dÃ©montre un fait que câ€™est vrai et que Ã§a vaut plus que mon jugement personnel Â». Pourtant, 69 % pensent toujours que la science est essentielle pour rÃ©pondre aux grands enjeux contemporains.</p>
      
          <p>La confiance envers les chercheurs reste Ã©levÃ©e : 75 % pour le secteur public et 69 % pour le secteur privÃ©. Cependant, des signaux prÃ©occupants Ã©mergent. La proportion de FranÃ§ais estimant bien comprendre les rÃ©sultats scientifiques est passÃ©e de 55 % Ã  50 %. Plus alarmant encore, 78 % des sondÃ©s trouvent de plus en plus difficile de distinguer les vraies informations scientifiques des fausses. Cette confusion alimente lâ€™adhÃ©sion croissante Ã  des idÃ©es infondÃ©es, comme la nÃ©gation du rÃ´le des humains dans le rÃ©chauffement climatique ou encore lâ€™impact supposÃ© de la 5G sur le systÃ¨me immunitaire.</p>
      
          <p>Le chiffre qui mâ€™a particuliÃ¨rement interpellÃ©, câ€™est que plus de la moitiÃ© des FranÃ§ais affirment que leur jugement personnel prÃ©vaut, mÃªme face aux faits dÃ©montrÃ©s par un spÃ©cialiste. Cette mÃ©fiance provient en partie du doute concernant lâ€™indÃ©pendance des scientifiques. Selon une Ã©tude rÃ©alisÃ©e par Cluster17 [2] pour <em>Le Point</em>, 56 % des FranÃ§ais estiment que les scientifiques ne sont pas indÃ©pendants et se laissent influencer par des groupes de pression. On assiste Ã  lâ€™Ã©mergence dâ€™un scepticisme Â« Ã©clairÃ© Â» au sein des classes moyennes et des diplÃ´mÃ©s intermÃ©diaires, particuliÃ¨rement prudents face aux innovations scientifiques. Face Ã  ces doutes, les FranÃ§ais prÃ©fÃ¨rent largement sâ€™informer auprÃ¨s de leurs proches (71 %), loin devant les mÃ©dias (31 %, en baisse de 13 points) et le gouvernement (28 %). On voit dans chaque Ã©tude consultÃ©e [1,2,3] une attitude paradoxale envers la science, oscillant entre confiance et dÃ©fiance.</p>
      
          <p>Pour Ã©viter que la recherche ne devienne un monde hermÃ©tique rÃ©servÃ© aux initiÃ©s, il semble primordial d'insister dÃ¨s lâ€™Ã©cole sur les principes du processus scientifique. En tant que doctorant, je pense quâ€™il est nÃ©cessaire de consacrer un temps substantiel Ã  rendre accessible son travail pour quâ€™il ait le plus dâ€™impact possible, tant dans le monde scientifique que dans les consciences de la population. Câ€™est ce travail qui prÃ©servera la vÃ©ritÃ© scientifique en France et Ã©vitera les dÃ©rives observÃ©es ailleurs, notamment aux Ã‰tats-Unis.</p>
      
          <p class="subtle small">
            [1] Ipsos, Institut Sapiens (2022, 2024). <em>BaromÃ¨tre Science et SociÃ©tÃ©</em>.<br>
            [2] Cluster17 (2025). <em>Les FranÃ§ais et la science : un rapport marquÃ© par des fractures et des paradoxes</em>.<br>
            [3] CNRS Journal (2022). <em>Les FranÃ§ais et la science : une relation ambivalente</em>.
          </p>
        `
      },
      
      { 
        title: "Je parle rÃ©guliÃ¨rement Ã  un canard", 
        img: "articles_images/article15.png", 
        text: `
          <p>Cette semaine on parle de cette recherche qui fait ğ—©ğ—œğ—•ğ—¥ğ—˜ğ—¥, qui nous Ã©moustille et des techniques un peu bizarres mais efficaces pour trouver ses bugs.</p>
      
          <p>Ã‡a fait trois mois que jâ€™ai commencÃ© ma thÃ¨se. Le dÃ©but Ã©tait intense : on finalisait la publication de mon travail de stage (<a href="https://lnkd.in/emDgcjmz" target="_blank">Ã  lire ici</a>). Depuis, le rythme a changÃ©. Je suis passÃ© Ã  une phase plus floue, plus exploratoire. Beaucoup de tests. Beaucoup dâ€™hypothÃ¨ses. Etâ€¦ beaucoup de bugs.</p>
      
          <p>Pour ceux qui, comme moi, ne connaissaient pas encore (merci JÃ©rÃ©my Rapin ğŸ’¡), voici la meilleure stratÃ©gie quâ€™on donne pour dÃ©bugger un code : le canard en plastique ğŸ¦†. Tu expliques ton bug Ã  un canard posÃ© sur ton bureau. Le simple fait de formuler Ã  voix haute suffit, dans la majoritÃ© des cas, Ã  en dÃ©bloquer la cause.</p>
      
          <p>Mais parfois â€“ heureusement â€“ il y a des moments oÃ¹ tu lances un script et Ã§a marche. Pas forcÃ©ment parfaitement mais juste assez pour que quelque chose se passe. Une courbe qui change. Une mÃ©trique qui grimpe. Une intuition qui se confirme.</p>
      
          <p>Ce moment dâ€™excitation, justifie Ã  lui seul lâ€™envie de faire de la recherche. Un moment de calme et de satisfaction. Au milieu du chaos des notebooks, des checkpoints et des logs : un court instant oÃ¹ on a le sentiment dâ€™avoir compris quelque chose. Les chercheurs nâ€™ont rien Ã  envier aux mÃ©tiers les plus palpitants. La recherche fait vibrer !</p>
      
          <p>Pendant une thÃ¨se, aller voir les rÃ©sultats dâ€™un script lancÃ© la veille, câ€™est un vrai moment palpitant. Dâ€™autant plus quâ€™en tant quâ€™Ã©tudiant, on ne comprend pas encore tout donc on est souvent un peu trop excitÃ© : chaque rÃ©sultat paraÃ®t potentiellement rÃ©volutionnaire. (Alors quâ€™il est souvent soit dÃ©jÃ  bien connu depuis 20 ansâ€¦ soit juste dÃ» Ã  un bug). On est peut-Ãªtre un peu trop excitÃ©s â€” mais câ€™est aussi Ã§a qui est beau.</p>
      
          <p>Les chercheurs plus expÃ©rimentÃ©s semblent sâ€™assagir avec le temps. Certains semblent rÃ©signÃ©s mais beaucoup gardent cette conviction que ğ—¾ğ˜‚ğ—²ğ—¹ğ—¾ğ˜‚ğ—² ğ—°ğ—µğ—¼ğ˜€ğ—² ğ—±ğ—² ğ—³ğ—¼ğ—¿ğ—ºğ—¶ğ—±ğ—®ğ—¯ğ—¹ğ—² ğ—²ğ˜€ğ˜ ğ—½ğ—¼ğ˜€ğ˜€ğ—¶ğ—¯ğ—¹ğ—² â€” je pense quâ€™il faut ğ—°ğ—µğ—²Ìğ—¿ğ—¶ğ—¿ cette conviction. La garder vivante aussi longtemps que possible.</p>
      
          <p>Et vous ? C'est quoi votre moment prÃ©fÃ©rÃ© en recherche ?</p>
        `
      },
      { 
        title: "Conference survival guide for a PhD student", 
        img: "articles_images/article16.png", 
        text: `
          <p>This week, I am in Singapore at the <em>#ICLR2025</em> conference.</p>
      
          <p>It was my first big AI conference, and I wanted to share some <strong>tips and tricks</strong> that older PhD students and my supervisors shared with me â€” super useful for newcomers like me. Attending a conference of this size is intense: hundreds of posters, talks everywhere, people everywhere. At first, it feels impossible to know what to do or who to talk to.</p>
      
          <p><strong>1 â€” Have a plan!</strong><br>
          Before the conference, list the posters you really want to see and the people you'd love to meet. Often, these large conferences offer nice visualizations to help you find interesting papers in your area. For <em>#ICLR</em>, Hendrik Strobelt and Benjamin Hoover [1] created a really helpful visualization tool for exploring the projects.</p>
      
          <p><strong>2 â€” Donâ€™t feel guilty if youâ€™re not coding or writing.</strong><br>
          Itâ€™s useless to attend a conference if you are constantly thinking about your ongoing projects. Of course, sometimes we have deadlines, but conferences are about opening doors and discovering knowledge you didnâ€™t even know existed. Itâ€™s messy, but valuable. You decided to attend â€” so enjoy it as much as possible.</p>
      
          <p><strong>3 â€” Prepare for the full zoo of conference characters ğŸ­</strong><br>
          The Hater: who thinks your project should have never existed, and maybe that you shouldnâ€™t even be doing research at all.<br>
          The Rockstar: the famous researcher everyone knows, always surrounded by a little crowd.<br>
          The Social Butterfly: doesnâ€™t really care much about posters or talks, but knows everyone and gets invited to all the networking events.<br>
          The Exhausted PhD: attends every single session, collects 50 PDFs to read later (never will), and takes notes on every poster they've seen.<br>
          The Recruit: secretly (?) trying to charm and get hired by your lab. Be kind to them. Remember, at some point, you will be this character too.<br>
          The Curious One: the kind of researcher you want to become â€” curious, open, respectful, and genuinely excited about science.</p>
      
          <p><strong>Final advice ğŸŒŸğŸŒŸ</strong><br>
          Everyone is here to discuss, so donâ€™t be afraid to say hi! Especially to people you admire.</p>
      
          <p>If you're heading to a conference soon, I hope this will help a little. It helped me a lot. âœŒï¸</p>
      
          <p class="subtle small">
            [1] Strobelt, H. & Hoover, B. (2022). <em>Interactive Corpora Visualization for 60 Years of AI Research</em>. Proceedings of the NeurIPS 2021 Competitions and Demonstrations Track.
          </p>
        `
      },
      
      { 
        title: "The art of communication in research", 
        img: "articles_images/article17.png", 
        text: `
          <p>This week, Iâ€™ve been reflecting on a delicate tension: <strong>how to make a research project visible without distorting its meaning</strong>.</p>
      
          <p>Every week, I post a snapshot of what makes me think, question, or smile during my PhD journey. Starting this week, these reflections will also be posted on Substack. Subscribe <a href="https://lnkd.in/eDEzWuti" target="_blank">here</a> to receive them directly in your inbox.</p>
      
          <p>In research, doing the work isnâ€™t enough. You also need to show that it was done and make its value legible to others. Without visibility, even meaningful contributions risk being forgotten. But, in trying to be understood, itâ€™s easy to overstate. Somewhere between silence and hype lies honest communication â€” and thatâ€™s not always easy to find. People have very limited time to give to your study, and if it receives media coverage, the message will likely be oversimplified and amplified to reach the broadest audience.</p>
      
          <p>This situation came into sharp focus recently when my team published a new paper [1]. The topic â€” neuroscience â€” and the affiliation â€” Meta â€” drew immediate media attention. But soon, some articles misrepresented our work: claiming we had achieved â€œmind reading,â€ when in fact we had decoded typed sentences from participants under very specific, controlled conditions. But once a message leaves your hands, people see what they want to see.</p>
      
          <p><strong>The lesson?</strong> Say what you did. Say what it means. Say what it doesnâ€™t mean. And then stop.</p>
      
          <p><strong>Some practical rules for honest communication I try to follow:</strong><br>
          1/ One paper = one strong, memorable idea. Make your Figure 1 count.<br>
          2/ Be ready to pitch even unfinished work â€” early feedback sharpens your vision.<br>
          3/ Leave breadcrumbs, not confetti. Let your path speak clearly over time.</p>
      
          <p>Maxwell Forbes [2] reframes the research paper not just as a report, but also as an advertisement:<br>
          1/ A currency (for conferences, grants, and jobs)<br>
          2/ A brand-builder (shaping how you're perceived)<br>
          3/ An ad (selling one idea to readers with limited time)</p>
      
          <p>Itâ€™s a powerful lens â€” but a dangerous one too. The temptation to polish the image beyond the substance is real. And yet, the responsibility remains: we owe readers clarity without distortion, and we owe ourselves the discipline of restraint. An easy way to overcome these issues? <strong>Reproducibility</strong>, of course.</p>
      
          <p class="subtle small">
            [1] LÃ©vy et al. (2025). <em>Brain-to-Text Decoding: A Non-invasive Approach via Typing</em>.<br>
            [2] Maxwell Forbes (2025). <em>The PhD Metagame: Your Paper is an Ad</em>.
          </p>
        `
      },
      
      { 
        title: "Neuroscience is weird (and thatâ€™s why we love it)", 
        img: "articles_images/article18.png", 
        text: `
          <p>This week, I took some time off and revisited a few neuroscience studies that are as surprising as they are insightful. Some even won Ig Nobel prizes: awards for research that â€œfirst makes you laugh, then makes you think.â€</p>
      
          <p><strong>1/ Seeing faces everywhere is a survival mechanism [1]</strong><br>
          That face you spot in a tree trunk, on your morning tartine, or in the clouds? Itâ€™s pareidolia. Your brain is wired to detect faces, even when there is little chance itâ€™s a real one. More recent studies [5] suggest men are more prone to it than women. Why? From an evolutionary perspective, a false alarm is less costly than missing a real threat.</p>
      
          <p><strong>2/ Swearing increases pain tolerance [2]</strong><br>
          In this study, participants were asked to keep their hands in ice water for as long as they could. They lasted significantly longer when allowed to swear. But only real swear words worked â€” not neutral words or fake ones like â€œfouch.â€ Swearing activates emotional brain circuits and reduces pain perception. It seems that letting it out really does help.</p>
      
          <p><strong>3/ Chocolate tastes better with the right music [3]</strong><br>
          Soft jazz can make chocolate taste sweeter. Harsh sounds make it taste more bitter. This is called crossmodal correspondence: your brain blends sound and taste. OK for music but it goes beyond that â€” flavour isnâ€™t just on the tongue. Smell, vision, touch â€” they all contribute. Maybe thatâ€™s why your grandmaâ€™s recipe never tastes quite the same today? Not the same smell in the house, not the same light, not the same sounds. Perception is a full-body experience.</p>
      
          <p><strong>4/ â€œHuh?â€ is a universal word [4]</strong><br>
          Researchers found that the sound â€œhuh?â€ exists in every language. Itâ€™s a universal repair signal. Itâ€™s your brainâ€™s way of saying: â€œWait, somethingâ€™s off.â€ Like a shared human error 404.</p>
      
          <p>All four of these studies remind me of the same thing: the brain isnâ€™t a perfect logic machine. Itâ€™s a living, improvising system. It was built for meaning, not truth.</p>
      
          <p>Would love to hear if youâ€™ve come across neuroscience studies that made you laugh or rethink something!</p>
      
          <p class="subtle small">
            [1] Liu et al. (2014). <em>Seeing Jesus in toast: Neural and behavioral correlates of face pareidolia</em>.<br>
            [2] Stephens et al. (2020). <em>Swearing as a Response to Pain</em>.<br>
            [3] Crisinel et al. (2012). <em>A bittersweet symphony: Systematically modulating the taste of food by changing the sonic properties of the soundtrack playing in the background</em>.<br>
            [4] Dingemanse et al. (2013). <em>Is â€œHuh?â€ a universal word?</em> <em>Conversational Infrastructure and the Convergent Evolution of Linguistic Items</em>.<br>
            [5] Wardle et al. (2022). <em>Illusory faces are more likely to be perceived as male than female</em>.<br>
            (Illustration)
          </p>
        `
      },
      
      { 
        title: "Rethinking how we hire researchers", 
        img: "articles_images/article19.png", 
        text: `
          <p>AI researchers today write less code and produce more experiments. Thatâ€™s not laziness. Itâ€™s a shift in the job itself.</p>
      
          <p>In AI research, and more broadly, in any research area that involves coding, the landscape has changed. Just a few years ago, testing one idea a day felt productive. Now, running through five or even ten variants in a single afternoon isnâ€™t unusual. Thatâ€™s thanks to our tight integration with LLMs. You sketch an idea, describe it, and the AI handles the boilerplate, the syntax, and even the silent bugs. Exit the wasted afternoons caused by a missing comma. Prompt well and youâ€™ve got working code in minutes.</p>
      
          <p>You probably know this already. But while our workflows have evolved, recruiting hasnâ€™t always caught up. So, hereâ€™s a question: should we still prioritize technical depth when hiring, or focus more on scientific thinking and problem-solving?</p>
      
          <p>Understanding what youâ€™re doing matters. But beyond that, the real differentiators are probably how well you think, how creatively you approach problems, and a newcomer: how fast you can iterate. I can see it every day, the most effective researchers arenâ€™t necessarily the most â€œtechnical,â€ but theyâ€™re sharp, fast, and precise.</p>
      
          <p>Yet many of us are still shy with LLMs. We open ChatGPT in a side tab. We hide the window when someone walks by. A recent study [1] even showed that using LLMs at work is perceived as a sign of weakness. But if you really understand your task, and code is the tool, then LLMs can enhance your productivity tenfold, especially when youâ€™re a junior.</p>
      
          <p>Thatâ€™s why we need to assess people based on how they actually work. Is it still relevant to make candidates memorize hundreds of Leetcode problems theyâ€™ll never use again? Thankfully, itâ€™s already partially reflected in current pipelines. AI Research Design Interviews, for example, are a step in the right direction, asking candidates to explore real research questions. But the pure coding component might need an update.</p>
      
          <p>It reminds me of how exams changed during COVID: when everyone had internet access, memorization didnâ€™t matter as much anymore. Understanding did.</p>
      
          <p>Every week, I share something that makes me think, smile, or question things during my PhD journey. These reflections are now on Substack too. You can subscribe <a href="https://lnkd.in/eDEzWuti" target="_blank">here</a> to receive them directly in your inbox.</p>
      
          <p class="subtle small">
            [1] Restrepo Amariles, D. (2025). <em>Usage cachÃ© de ChatGPT</em>.<br>
            [2] Preston, D. (2025). <em>Trumpâ€™s new tariff math looks a lot like ChatGPTâ€™s</em>. The Verge.<br>
            Illustration : <em>The Internship</em> (2013).
          </p>
        `
      },
      
      { 
        title: "Discipline or quality?", 
        img: "articles_images/article20.png", 
        text: `
          <p>After 20 weeks of weekly posts, I wanted to say thank you. Writing these posts and connecting with many of you has been both energizing and meaningful. It's been a pleasant surprise to see that there is room for this kind of content on LinkedIn.</p>
      
          <p>Iâ€™ll be moving to a monthly long-form format on Substack, with shorter summaries shared here. This change is about finding a better rhythm allowing to write with more depth and clarity. Between LinkedIn's character limits and the time constraints of a PhD, the weekly format has started to feel limiting.</p>
      
          <p>Beyond the value of this PhD logbook, I still believe itâ€™s possible to share ideas that are nuanced, well-sourced, and grounded in reality. Even at a time when those values feel increasingly rare. Thatâ€™s what Iâ€™ll keep aiming for, just at a slower and more intentional pace.</p>
      
          <p>Thanks again for following along. I hope to see you over there.<br>
          Substack: <a href="https://lnkd.in/eDEzWuti" target="_blank">https://lnkd.in/eDEzWuti</a></p>
        `
      },
  ];